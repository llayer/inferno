{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inferno_TF1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMsgrVmiuTz1I0swOoD62tQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/llayer/inferno/blob/master/inferno_TF1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dos0UMhvLxnd",
        "colab_type": "text"
      },
      "source": [
        "#INFERNO with TF 1.x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlhe0kNDA55Q",
        "colab_type": "text"
      },
      "source": [
        "## Install the old tensorflow versions and the required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ntc5Urbt1knH",
        "colab_type": "code",
        "outputId": "439b3f7a-c508-456f-fa45-9f46fa8dc75f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        }
      },
      "source": [
        "!pip install tensorflow==1.10.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/7e/a484776c73b1431f2b077e13801531e966113492552194fe721e6ef88d5d/tensorflow-1.10.1-cp36-cp36m-manylinux1_x86_64.whl (58.4MB)\n",
            "\u001b[K     |████████████████████████████████| 58.4MB 73kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1) (0.34.2)\n",
            "Collecting setuptools<=39.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/10/79282747f9169f21c053c562a0baa21815a8c7879be97abd930dbcf862e8/setuptools-39.1.0-py2.py3-none-any.whl (566kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 46.8MB/s \n",
            "\u001b[?25hCollecting numpy<=1.14.5,>=1.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/1e/116ad560de97694e2d0c1843a7a0075cc9f49e922454d32f49a80eb6f1f2/numpy-1.14.5-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n",
            "\u001b[K     |████████████████████████████████| 12.2MB 45.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1) (1.28.1)\n",
            "Collecting tensorboard<1.11.0,>=1.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/17/ecd918a004f297955c30b4fffbea100b1606c225dbf0443264012773c3ff/tensorboard-1.10.0-py3-none-any.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 44.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1) (0.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1) (0.3.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1) (3.10.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.1) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.1) (1.0.1)\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement setuptools>=41.2, but you'll have setuptools 39.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: umap-learn 0.4.0 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement numpy>=1.16.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: numba 0.48.0 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-auth 1.7.2 has requirement setuptools>=40.3.0, but you'll have setuptools 39.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.60 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.29 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: blis 0.4.1 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.0.1.post1 has requirement numpy>=1.16, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: setuptools, numpy, tensorboard, tensorflow\n",
            "  Found existing installation: setuptools 46.1.3\n",
            "    Uninstalling setuptools-46.1.3:\n",
            "      Successfully uninstalled setuptools-46.1.3\n",
            "  Found existing installation: numpy 1.18.2\n",
            "    Uninstalling numpy-1.18.2:\n",
            "      Successfully uninstalled numpy-1.18.2\n",
            "  Found existing installation: tensorboard 2.2.0\n",
            "    Uninstalling tensorboard-2.2.0:\n",
            "      Successfully uninstalled tensorboard-2.2.0\n",
            "  Found existing installation: tensorflow 2.2.0rc2\n",
            "    Uninstalling tensorflow-2.2.0rc2:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc2\n",
            "Successfully installed numpy-1.14.5 setuptools-39.1.0 tensorboard-1.10.0 tensorflow-1.10.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dpI_G7X2PQN",
        "colab_type": "code",
        "outputId": "a05444b8-61e8-4216-fe4e-ec7450de2974",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "!pip install tensorflow-probability==0.3.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-probability==0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/61/401c24a0062f406d5b8d9014210970f977a571225543d151fdebbeab42a1/tensorflow_probability-0.3.0-py2.py3-none-any.whl (509kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.3.0) (1.12.0)\n",
            "Requirement already satisfied: tensorflow>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.3.0) (1.10.1)\n",
            "Requirement already satisfied: numpy<=1.14.5,>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.3.0) (1.14.5)\n",
            "Requirement already satisfied: tensorboard<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow-probability==0.3.0) (1.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow-probability==0.3.0) (1.28.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow-probability==0.3.0) (0.34.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow-probability==0.3.0) (0.9.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow-probability==0.3.0) (0.3.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow-probability==0.3.0) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow-probability==0.3.0) (3.10.0)\n",
            "Requirement already satisfied: setuptools<=39.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow-probability==0.3.0) (39.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow-probability==0.3.0) (0.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow>=1.10.0->tensorflow-probability==0.3.0) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow>=1.10.0->tensorflow-probability==0.3.0) (1.0.1)\n",
            "Installing collected packages: tensorflow-probability\n",
            "  Found existing installation: tensorflow-probability 0.9.0\n",
            "    Uninstalling tensorflow-probability-0.9.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.9.0\n",
            "Successfully installed tensorflow-probability-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQE8UZXr7IXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import trange"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcnCg-bW1rOw",
        "colab_type": "code",
        "outputId": "7afb7adc-c275-46cb-d193-d27a2a9cc049",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUMwFsKx2BF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ge = tf.contrib.graph_editor\n",
        "ds = tfp.distributions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9l614mn2JBY",
        "colab_type": "code",
        "outputId": "8f4a805b-c650-4b09-918e-5b4d1b2c47c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "!pip install git+https://github.com/pablodecm/neyman.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/pablodecm/neyman.git\n",
            "  Cloning https://github.com/pablodecm/neyman.git to /tmp/pip-req-build-xrnsxoyw\n",
            "  Running command git clone -q https://github.com/pablodecm/neyman.git /tmp/pip-req-build-xrnsxoyw\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from neyman==0.0.1) (1.14.5)\n",
            "Collecting edward>=1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/59/5831fa77a01a7b16d53626966b06e89064b1696166d74e856ef3eefb3119/edward-1.3.5.tar.gz (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 2.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from edward>=1.3->neyman==0.0.1) (1.12.0)\n",
            "Building wheels for collected packages: neyman, edward\n",
            "  Building wheel for neyman (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neyman: filename=neyman-0.0.1-cp36-none-any.whl size=5293 sha256=47dfc6da08061c57057d150c0f8d4fb9ab3532f355a1e5f2d557ada40d53277c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-nxxrxo2h/wheels/c5/02/c3/daef8b8e644e4dbee14ad489db8f51c08838dfcc1a621ed3ea\n",
            "  Building wheel for edward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for edward: filename=edward-1.3.5-cp36-none-any.whl size=90388 sha256=6f3f0db6c4fcf598c9392411494e1493dbc0f2de6e465e4361ca4ef815c37f8d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/61/0c/1f36f3f0c629d1b7a24d042d2c37015a66c091729c95dd8425\n",
            "Successfully built neyman edward\n",
            "Installing collected packages: edward, neyman\n",
            "Successfully installed edward-1.3.5 neyman-0.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/google/colab/_pip.py:87: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.6/dist-packages/edward-1.3.5.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
            "  for line in open(toplevel):\n",
            "/usr/local/lib/python3.6/dist-packages/google/colab/_pip.py:87: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.6/dist-packages/neyman-0.0.1.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
            "  for line in open(toplevel):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx0qwbnG3U-T",
        "colab_type": "code",
        "outputId": "fa3286b4-16e0-4b17-e6d4-381e1714fd26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz5W-NMz3WSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from neyman.inferences import batch_hessian"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnLpbo443dkF",
        "colab_type": "code",
        "outputId": "3083af37-eb02-486f-fa0e-3f2c33cd3f10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "! git clone https://github.com/llayer/paper-inferno"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'paper-inferno'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 1063 (delta 8), reused 7 (delta 0), pack-reused 1048\u001b[K\n",
            "Receiving objects: 100% (1063/1063), 1.56 MiB | 1.67 MiB/s, done.\n",
            "Resolving deltas: 100% (687/687), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC2Jd8aC5ML0",
        "colab_type": "code",
        "outputId": "8e82023b-7f54-4114-d4a5-193d6f3b4fef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd paper-inferno/"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/paper-inferno\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wqA7hmq5uHn",
        "colab_type": "code",
        "outputId": "cb8e4df3-5769-4a7f-b1d4-e85526e4142a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "code  environment.yml  LICENSE\tnotebooks  paper  README.md  setup.cfg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSjOsOJd5vLu",
        "colab_type": "code",
        "outputId": "34f4c8fb-daac-4741-edca-5cf9a9a48624",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd code"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/paper-inferno/code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdBX_DHc5ykP",
        "colab_type": "code",
        "outputId": "5ef0b722-e6c5-40a7-8aed-919c89c2f6a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "benchmarking.py\t\t       synthetic_3D_inferno.py\n",
            "extended_model.py\t       synthetic_3D_train_cross_entropy.py\n",
            "fisher_matrix.py\t       synthetic_3D_train_inferno.py\n",
            "summary_statistic_computer.py  template_likelihood.py\n",
            "synthetic_3D_cross_entropy.py  template_model.py\n",
            "synthetic_3D_example.py        train_helpers.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OETqDsfvBG7Q",
        "colab_type": "text"
      },
      "source": [
        "## Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpqtILTY5zlN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from neyman.inferences import batch_hessian\n",
        "\n",
        "from synthetic_3D_example import SyntheticThreeDimExample\n",
        "from train_helpers import MixtureBatcher\n",
        "import os\n",
        "import json\n",
        "import itertools as it\n",
        "from fisher_matrix import FisherMatrix\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl82hwRL53D1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = tf.keras\n",
        "ds = tfp.distributions\n",
        "ge = tf.contrib.graph_editor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cpl7RlN46Jna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SyntheticThreeDimInferno(object):\n",
        "\n",
        "  def __init__(self, model_path, poi, pars, seed, aux={}):\n",
        "\n",
        "    tf.set_random_seed(seed)\n",
        "\n",
        "    self.problem = SyntheticThreeDimExample()\n",
        "\n",
        "    self.batcher = MixtureBatcher([\"sig\", \"bkg\"])\n",
        "\n",
        "    s_batch = self.batcher.batch[\"sig\"]\n",
        "    b_batch = self.problem.transform_bkg(self.batcher.batch[\"bkg\"])\n",
        "    b_sizes = [tf.shape(s_batch)[0], tf.shape(b_batch)[0]]\n",
        "    train_batch = tf.concat([s_batch, b_batch], axis=0,\n",
        "                            name=\"input_batch\")\n",
        "\n",
        "    k_init = \"he_normal\"\n",
        "    Dense = k.layers.Dense\n",
        "    self.nn_model = k.Sequential([Dense(units=100, activation=\"relu\",\n",
        "                                        kernel_initializer=k_init,\n",
        "                                        input_shape=(3,)),\n",
        "                                  Dense(units=100, activation=\"relu\",\n",
        "                                        kernel_initializer=k_init),\n",
        "                                  Dense(units=10, activation=\"linear\")])\n",
        "\n",
        "    self.logits = self.nn_model(train_batch)\n",
        "    self.temperature = tf.placeholder_with_default(1., shape=())\n",
        "    self.probs = tf.nn.softmax(self.logits / self.temperature)\n",
        "\n",
        "    s_probs, b_probs = tf.split(self.probs, b_sizes, axis=0)\n",
        "\n",
        "    s_counts = tf.reduce_mean(s_probs, axis=0)\n",
        "    b_counts = tf.reduce_mean(b_probs, axis=0),\n",
        "\n",
        "    self.exp_counts = tf.cast(self.problem.s_exp * s_counts +\n",
        "                              self.problem.b_exp * b_counts,\n",
        "                              dtype=tf.float64)\n",
        "\n",
        "    self.pois = ds.Poisson(self.exp_counts, name=\"poisson\")\n",
        "    self.asimov = tf.stop_gradient(self.exp_counts, name=\"asimov\")\n",
        "\n",
        "    self.nll = - tf.cast(tf.reduce_sum(self.pois.log_prob(self.asimov)),\n",
        "                         name=\"nll\", dtype=tf.float32)\n",
        "\n",
        "    all_pars = list(self.problem.all_pars.values())\n",
        "    self.hess_nll, self.grad_nll = batch_hessian(self.nll,\n",
        "                                                 all_pars)\n",
        "\n",
        "    self.aux = aux\n",
        "\n",
        "    self.nll_aux = {}\n",
        "    self.hess_nll_aux = {}\n",
        "    for par, dist in self.aux.items():\n",
        "        self.nll_aux[par] = -dist.log_prob(self.problem.all_pars[par])\n",
        "        self.hess_nll_aux[par], _ = batch_hessian(self.nll_aux[par],\n",
        "                                                  all_pars)\n",
        "\n",
        "    self.ext_nll = sum([self.hess_nll] + list(self.hess_nll_aux.values()))\n",
        "\n",
        "    self.cov_nll = self.cov_matrix(pars)\n",
        "    idx_poi = pars.index(poi)\n",
        "    self.loss = self.cov_nll[idx_poi, idx_poi]\n",
        "\n",
        "    # remove stop gradient after loss is computed\n",
        "    ge.edit.bypass(self.asimov.op)\n",
        "\n",
        "    self.lr = tf.placeholder(shape=(), dtype=tf.float32)\n",
        "    self.optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
        "    self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "    self.train_op = self.optimizer.minimize(\n",
        "        self.loss, global_step=self.global_step)\n",
        "\n",
        "    self.init_op = tf.global_variables_initializer()\n",
        "\n",
        "    self.model_path = model_path\n",
        "\n",
        "    if not os.path.exists(self.model_path):\n",
        "      os.makedirs(self.model_path)\n",
        "\n",
        "    json_str = self.nn_model.to_json()\n",
        "    with open(f'{self.model_path}/model.json', 'w') as f:\n",
        "      json.dump(json_str, f)\n",
        "\n",
        "    self.saver = tf.train.Saver()\n",
        "    self.history = {}\n",
        "\n",
        "  def cov_matrix(self, pars):\n",
        "\n",
        "    pars = tuple(pars)\n",
        "\n",
        "    indices = [list(self.problem.all_pars.keys()).index(par) for par in pars]\n",
        "    idx_subset = np.reshape(list(it.product(indices, indices)),\n",
        "                            (len(pars), len(pars), -1))\n",
        "    hess_subset = tf.gather_nd(self.ext_nll, idx_subset)\n",
        "    cov_nll = tf.matrix_inverse(hess_subset)\n",
        "\n",
        "    return cov_nll\n",
        "\n",
        "  def fit(self, n_epochs, lr, temperature, batch_size, seed, par_phs={}):\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "      train_arrays = sess.run(self.problem.train_data())\n",
        "      valid_arrays = sess.run(self.problem.valid_data())\n",
        "\n",
        "    phs_train = {self.lr: lr,\n",
        "                 self.temperature: temperature}\n",
        "\n",
        "    phs_val = {self.temperature: temperature}\n",
        "\n",
        "    rs = np.random.RandomState(seed=seed)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "\n",
        "      #writer = tf.summary.FileWriter('logs', sess.graph)\n",
        "\n",
        "      k.backend.set_session(sess)\n",
        "      sess.run(self.init_op)\n",
        "      batch_n = 0\n",
        "      with trange(n_epochs) as t:\n",
        "        for i in t:\n",
        "          shuffle_seed = rs.randint(np.iinfo(np.int32).max)\n",
        "          self.batcher.init_iterator(train_arrays,\n",
        "                                     batch_size=batch_size, seed=shuffle_seed)\n",
        "          while True:\n",
        "            try:\n",
        "              batch_n += 1\n",
        "\n",
        "              \n",
        "              loss_t, _ = sess.run([self.loss, self.train_op], phs_train)\n",
        "              #print( loss_t )\n",
        "              #print( self.loss.eval() )\n",
        "\n",
        "              \"\"\"\n",
        "              loss_t = sess.run(self.loss, phs_train)\n",
        "              print( \"Loss in first session run\" )\n",
        "              print(self.loss.eval())\n",
        "              print( \"Cov first session run\" )\n",
        "              print(self.cov_nll.eval())\n",
        "              sess.run(self.train_op, phs_train)\n",
        "              print( \"Loss in second session run\" )\n",
        "              print(self.loss.eval())\n",
        "              print( \"Cov second session run\" )\n",
        "              print(self.cov_nll.eval())\n",
        "              \"\"\"\n",
        "              self.history.setdefault(\"loss_train\", []).append(\n",
        "                  [batch_n, float(np.sqrt(loss_t))])\n",
        "            except tf.errors.OutOfRangeError:\n",
        "              break\n",
        "          # fix seed for validation set (no need to shuffle)\n",
        "          self.batcher.init_iterator(valid_arrays,\n",
        "                                     batch_size=batch_size, seed=20)\n",
        "          val_losses = []\n",
        "          while True:\n",
        "            try:\n",
        "              loss_t = sess.run([self.loss], phs_val)\n",
        "              val_losses.append(np.sqrt(loss_t))\n",
        "            except tf.errors.OutOfRangeError:\n",
        "              break\n",
        "          val_loss = np.mean(val_losses)\n",
        "          val_loss_std = np.std(val_losses, ddof=1)\n",
        "          t.set_postfix({\"mean_val_loss\": val_loss})\n",
        "          self.history.setdefault(\"loss_valid\", []).append(\n",
        "              [batch_n, float(val_loss)])\n",
        "          self.history.setdefault(\"loss_std_valid\", []).append(\n",
        "              [batch_n, float(val_loss_std)])\n",
        "\n",
        "      self.nn_model.save_weights(f'{self.model_path}/model.h5')\n",
        "      self.saver.save(sess, f'{self.model_path}/model.ckpt',\n",
        "                      global_step=self.global_step)\n",
        "      with open(f'{self.model_path}/history.json', 'w') as fp:\n",
        "        json.dump(self.history, fp)\n",
        "\n",
        "  def load_weights(self):\n",
        "    sess = tf.get_default_session()\n",
        "    last_ckpt = tf.train.latest_checkpoint(f'{self.model_path}')\n",
        "    print(\"loading_vars_from\", last_ckpt)\n",
        "    self.saver.restore(sess, last_ckpt)\n",
        "\n",
        "  def eval_hessian(self, temperature):\n",
        "\n",
        "    phs_val = {self.temperature: temperature}\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "      valid_arrays = sess.run(self.problem.valid_data())\n",
        "      self.load_weights()\n",
        "      self.batcher.init_iterator(valid_arrays,\n",
        "                                 batch_size=-1, seed=20)\n",
        "      hess, hess_aux = sess.run([self.hess_nll, self.hess_nll_aux], phs_val)\n",
        "      print(hess)\n",
        "\n",
        "\n",
        "    pars = list(self.problem.all_pars.keys())\n",
        "    fisher = FisherMatrix(hess, pars)\n",
        "    aux_fisher = FisherMatrix(sum(hess_aux.values()), pars)\n",
        "\n",
        "    return fisher, aux_fisher"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lE_H_mXG6kj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "4ae2a709-1d8b-4d89-bf9b-7ad9355f3186"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "path = \"/content/gdrive/My Drive/model_tf1.h5\""
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgO0Ajic6Pe8",
        "colab_type": "code",
        "outputId": "0a2bfa6f-6ced-44ab-95c6-34c5b7f55b76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "n_epochs = 100\n",
        "lr = 1e-6\n",
        "batch_size = 1000\n",
        "t_train = 0.1\n",
        "t_eval = 0.05\n",
        "\n",
        "n_inits = 100\n",
        "seed = 7\n",
        "\n",
        "pars = [\"s_exp\", \"r_dist\", \"b_rate\"]\n",
        "\n",
        "aux = {\"r_dist\": ds.Normal(loc=2.0, scale=0.4),\n",
        "       \"b_rate\": ds.Normal(loc=3.0, scale=1.)}\n",
        "\n",
        "\n",
        "inf_path = path\n",
        "\n",
        "inferno = SyntheticThreeDimInferno(model_path=inf_path, poi=\"s_exp\",\n",
        "                                    pars=pars, seed=seed, aux=aux)\n",
        "\n",
        "inferno.fit(n_epochs=n_epochs, lr=lr, batch_size=batch_size,\n",
        "            temperature=t_train, seed=seed)\n",
        "\n",
        "inf_fisher, inf_aux_fisher = inferno.eval_hessian(temperature=t_eval)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "100%|██████████| 100/100 [23:12<00:00, 13.92s/it, mean_val_loss=17.1]\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loading_vars_from /content/gdrive/My Drive/model_tf1.h5/model.ckpt-10000\n",
            "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/model_tf1.h5/model.ckpt-10000\n",
            "[[ 3.9608134e-03 -2.1585055e-01 -7.0352688e-02  8.0193585e-04]\n",
            " [-2.1585131e-01  8.7026276e+01  2.3724300e+01  1.0792568e-02]\n",
            " [-7.0353150e-02  2.3724384e+01  1.4914366e+01  3.5176619e-03]\n",
            " [ 8.0193579e-04  1.0792408e-02  3.5180778e-03  9.5989980e-04]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BfLnWxOIOl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORACY_TvJECY",
        "colab_type": "text"
      },
      "source": [
        "## Templates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYSadygDJZg1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from template_model import TemplateModel\n",
        "from summary_statistic_computer import SummaryStatisticComputer\n",
        "from train_helpers import NumpyEncoder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRMcML617W-3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6e29c903-14a0-41b9-bf8b-d4fa60546b17"
      },
      "source": [
        "aux_std = [None, 0.4, 1.0, 100.]\n",
        "([\"s_exp\", \"r_dist\", \"b_rate\"], aux_std)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['s_exp', 'r_dist', 'b_rate'], [None, 0.4, 1.0, 100.0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixijW0xkJNVm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5afdfa4c-d3be-46c2-89ed-ae3ab1774c53"
      },
      "source": [
        "tm = TemplateModel()\n",
        "ssc = SummaryStatisticComputer()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4DdzRGyJWF-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sess = tf.Session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JEDpy3oKJf7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "outputId": "ddc51e40-a3bf-4212-80f7-f46e0b545c8e"
      },
      "source": [
        "with sess.as_default():\n",
        "    shapes = ssc.classifier_shapes(path, sess=sess)\n",
        "    with open(path + \"/templates.json\", 'w') as t_file:\n",
        "        json.dump({str(k): v for k, v in shapes.items()}, t_file, cls=NumpyEncoder)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rD5TGi0tKayF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        },
        "outputId": "86382484-b14a-47de-d782-f75b19f25ee7"
      },
      "source": [
        "shapes"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('bkg',\n",
              "  1.8,\n",
              "  2.5): array([93889,  4511,   888,   347,   167,   102,    48,    33,    12,\n",
              "            3]),\n",
              " ('bkg',\n",
              "  1.8,\n",
              "  3.0): array([94798,  4154,   634,   228,   105,    37,    32,     8,     3,\n",
              "            1]),\n",
              " ('bkg',\n",
              "  1.8,\n",
              "  3.5): array([95508,  3767,   472,   159,    49,    29,    12,     3,     1,\n",
              "            0]),\n",
              " ('bkg',\n",
              "  2.0,\n",
              "  2.5): array([94422,  4119,   786,   324,   178,    83,    48,    26,    11,\n",
              "            3]),\n",
              " ('bkg',\n",
              "  2.0,\n",
              "  3.0): array([95260,  3754,   597,   221,    98,    35,    21,    10,     3,\n",
              "            1]),\n",
              " ('bkg',\n",
              "  2.0,\n",
              "  3.5): array([95827,  3504,   448,   137,    44,    28,     7,     4,     1,\n",
              "            0]),\n",
              " ('bkg',\n",
              "  2.2,\n",
              "  2.5): array([94846,  3792,   737,   314,   158,    67,    52,    20,    12,\n",
              "            2]),\n",
              " ('bkg',\n",
              "  2.2,\n",
              "  3.0): array([95655,  3458,   533,   210,    73,    38,    21,     9,     2,\n",
              "            1]),\n",
              " ('bkg',\n",
              "  2.2,\n",
              "  3.5): array([96203,  3172,   432,   116,    40,    25,     9,     2,     1,\n",
              "            0]),\n",
              " ('sig',): array([55649, 28950,  7395,  3518,  1836,  1115,   707,   468,   269,\n",
              "           93])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W55UG8Y1BUBv",
        "colab_type": "text"
      },
      "source": [
        "## Profile likelihood"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3xrILSnKe3e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0ae50c71-5717-4508-ba28-ea924daee507"
      },
      "source": [
        "tm = TemplateModel(multiple_pars=True)\n",
        "\n",
        "step_size = 0.1\n",
        "n_steps = 100\n",
        "d = 0.0\n",
        "pars = [\"r_dist\",\"b_rate\"]\n",
        "\n",
        "s_exp_scan = np.linspace(20.,80.,61, endpoint=True)\n",
        "par_phs = {tm.r_dist : 2.0*np.ones_like(s_exp_scan),\n",
        "           tm.b_rate : 3.0*np.ones_like(s_exp_scan),\n",
        "           tm.s_exp :  s_exp_scan,\n",
        "           tm.b_exp : 1000.0*np.ones_like(s_exp_scan)} \n",
        "\n",
        "\n",
        "#inf_pls = {}\n",
        "#inf_nlls = {}\n",
        "#for inf_path in glob(inf_template_path):\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    tm.templates_from_json(path + \"/templates.json\")\n",
        "\n",
        "    #print(tm.shape_phs)\n",
        "\n",
        "    #asimov_data = sess.run(tm.bkg_shape, {**tm.shape_phs})\n",
        "    asimov_data = tm.asimov_data(sess=sess)\n",
        "\n",
        "    #print(sess.run(tm.t_exp))\n",
        "\n",
        "    print(asimov_data)\n",
        "\n",
        "\n",
        "    obs_phs = {tm.obs : asimov_data}\n",
        "    mod_phs = par_phs.copy()\n",
        "    # get likelihood before changing pars\n",
        "    nll, sub_hess, sub_grad = tm.hessian_and_gradient(pars=pars,\n",
        "                                                      par_phs=mod_phs, obs_phs=obs_phs)\n",
        "    \n",
        "    print(nll)\n",
        "\n",
        "    \n",
        "    \n",
        "    # profile likelihood with Newton method\n",
        "    for i in range(n_steps):\n",
        "      newton_step =  np.matmul(np.linalg.inv(sub_hess+d*np.ones([len(pars)])),sub_grad[:,:,np.newaxis])\n",
        "\n",
        "      mod_phs[tm.r_dist] = mod_phs[tm.r_dist] - step_size*newton_step[:,0,0]\n",
        "      mod_phs[tm.b_rate] = mod_phs[tm.b_rate] - step_size*newton_step[:,1,0]\n",
        "\n",
        "      print(\"step\", i)\n",
        "      #print(mod_phs)\n",
        "\n",
        "      p_nll, sub_hess, sub_grad = tm.hessian_and_gradient(pars=[\"r_dist\",\"b_rate\"],\n",
        "                                                        par_phs=mod_phs, obs_phs=obs_phs)\n",
        "      \n",
        "      print(p_nll)\n",
        "    \n",
        "    print(\"nll - p_nll\",(p_nll-nll).sum())\n",
        "    "
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9.80424500e+02 5.20149994e+01 9.66749954e+00 3.96899986e+00\n",
            " 1.89800000e+00 9.07500029e-01 5.63499987e-01 3.33999991e-01\n",
            " 1.64499998e-01 5.64999953e-02]\n",
            "[16.879148  16.754284  16.63483   16.520676  16.411713  16.307835\n",
            " 16.208944  16.114946  16.025745  15.9412565 15.861392  15.78607\n",
            " 15.71521   15.648736  15.586574  15.52865   15.4748955 15.425242\n",
            " 15.379625  15.337979  15.300245  15.26636   15.236268  15.20991\n",
            " 15.187234  15.168183  15.152707  15.140755  15.132276  15.127223\n",
            " 15.125549  15.127208  15.132155  15.140347  15.15174   15.166294\n",
            " 15.183968  15.2047205 15.228516  15.255314  15.285078  15.317772\n",
            " 15.353361  15.39181   15.433085  15.477153  15.523981  15.573537\n",
            " 15.625791  15.680711  15.738267  15.798431  15.861174  15.926467\n",
            " 15.994282  16.064594  16.137373  16.212597  16.290237  16.370268\n",
            " 16.45267  ]\n",
            "step 0\n",
            "[16.888147  16.423225  16.352623  16.274076  16.193302  16.113026\n",
            " 16.034605  15.958769  15.885922  15.816295  15.750021  15.687173\n",
            " 15.627781  15.571855  15.51939   15.470365  15.424755  15.382531\n",
            " 15.343661  15.308107  15.275836  15.246808  15.220988  15.198338\n",
            " 15.1788225 15.162403  15.149047  15.138717  15.13138   15.127001\n",
            " 15.125549  15.12699   15.131295  15.138432  15.14837   15.16108\n",
            " 15.176537  15.194708  15.2155695 15.239093  15.265253  15.294024\n",
            " 15.325382  15.359302  15.39576   15.434732  15.476197  15.520131\n",
            " 15.566513  15.615322  15.666534  15.720133  15.776095  15.834402\n",
            " 15.895033  15.957971  16.023193  16.090685  16.160427  16.232399\n",
            " 16.306587 ]\n",
            "step 1\n",
            "[16.669397  16.09728   16.119835  16.10048   16.044846  15.9794035\n",
            " 15.912549  15.847056  15.78395   15.723638  15.666282  15.611942\n",
            " 15.560629  15.512327  15.467019  15.424673  15.385262  15.348753\n",
            " 15.31512   15.284331  15.256357  15.231171  15.208743  15.189048\n",
            " 15.172059  15.15775   15.146096  15.137073  15.130655  15.1268215\n",
            " 15.125549  15.126815  15.130597  15.136875  15.145629  15.156837\n",
            " 15.170483  15.186543  15.205004  15.225843  15.249045  15.274591\n",
            " 15.302467  15.332654  15.365137  15.3999    15.436926  15.476204\n",
            " 15.517715  15.561447  15.607384  15.655513  15.705821  15.758295\n",
            " 15.81292   15.869683  15.928572  15.989576  16.05268   16.117874\n",
            " 16.185143 ]\n",
            "step 2\n",
            "[16.48557   15.943428  15.99629   15.981086  15.931045  15.873368\n",
            " 15.814862  15.757637  15.702499  15.649791  15.599653  15.552141\n",
            " 15.507265  15.465013  15.425368  15.388302  15.35379   15.321805\n",
            " 15.292321  15.265313  15.240757  15.218629  15.198911  15.181579\n",
            " 15.166615  15.153999  15.143715  15.135744  15.13007   15.126677\n",
            " 15.125549  15.126672  15.130031  15.135612  15.143401  15.153386\n",
            " 15.165554  15.179893  15.196388  15.215032  15.23581   15.258714\n",
            " 15.2837305 15.310851  15.340065  15.371363  15.404734  15.440171\n",
            " 15.477661  15.5171995 15.558776  15.602382  15.648007  15.695647\n",
            " 15.745292  15.796934  15.850564  15.9061775 15.963763  16.023317\n",
            " 16.084831 ]\n",
            "step 3\n",
            "[16.331547  15.857187  15.901935  15.885856  15.840197  15.788705\n",
            " 15.736807  15.686157  15.637384  15.590763  15.546405  15.504357\n",
            " 15.464625  15.427204  15.3920765 15.359219  15.328611  15.30023\n",
            " 15.274055  15.250062  15.228236  15.208555  15.191005  15.175568\n",
            " 15.16223   15.150975  15.141793  15.134671  15.129597  15.126559\n",
            " 15.125549  15.126556  15.129571  15.134585  15.141591  15.150579\n",
            " 15.161544  15.174477  15.18937   15.206219  15.225016  15.245756\n",
            " 15.268433  15.29304   15.319574  15.348029  15.378399  15.410683\n",
            " 15.444872  15.480966  15.518955  15.558843  15.600619  15.644283\n",
            " 15.689832  15.737261  15.786565  15.837744  15.890791  15.945706\n",
            " 16.002483 ]\n",
            "step 4\n",
            "[16.262873  15.793034  15.827811  15.809821  15.767654  15.721091\n",
            " 15.674443  15.629022  15.585319  15.543551  15.503806  15.466119\n",
            " 15.430497  15.396933  15.365412  15.3359165 15.308427  15.282926\n",
            " 15.259395  15.237816  15.218175  15.200454  15.184642  15.170726\n",
            " 15.158694  15.148536  15.140242  15.133803  15.129213  15.126464\n",
            " 15.125549  15.126462  15.129199  15.133753  15.1401205 15.148298\n",
            " 15.158283  15.170069  15.183656  15.19904   15.21622   15.235191\n",
            " 15.255955  15.278509  15.30285   15.32898   15.356896  15.3866\n",
            " 15.418087  15.451363  15.486421  15.523267  15.561896  15.602312\n",
            " 15.644514  15.688502  15.734276  15.781837  15.831185  15.882322\n",
            " 15.935246 ]\n",
            "step 5\n",
            "[15.84183   15.744096  15.7689705 15.749046  15.70968   15.667045\n",
            " 15.62458   15.583322  15.543657  15.505759  15.469694  15.435491\n",
            " 15.40315   15.372666  15.34403   15.317222  15.292228  15.26903\n",
            " 15.247618  15.227971  15.21008   15.193933  15.179517  15.166823\n",
            " 15.155842  15.146566  15.1389885 15.133102  15.128903  15.126388\n",
            " 15.125549  15.126387  15.128896  15.133077  15.138927  15.146445\n",
            " 15.155632  15.166485  15.179007  15.193196  15.209056  15.226586\n",
            " 15.245789  15.2666645 15.289218  15.31345   15.339363  15.3669615\n",
            " 15.396246  15.427221  15.459891  15.49426   15.530328  15.568101\n",
            " 15.607583  15.648777  15.691687  15.736317  15.782669  15.830748\n",
            " 15.880558 ]\n",
            "step 6\n",
            "[15.787586 15.706181 15.722026 15.700407 15.663291 15.623796 15.58467\n",
            " 15.54673  15.510289 15.47548  15.442354 15.410933 15.381215 15.353198\n",
            " 15.326867 15.302209 15.279212 15.257862 15.238145 15.22005  15.203566\n",
            " 15.18868  15.175386 15.163676 15.153542 15.144976 15.137976 15.132536\n",
            " 15.128654 15.126325 15.125549 15.126325 15.128651 15.132528 15.137958\n",
            " 15.14494  15.153478 15.163571 15.175225 15.188442 15.203226 15.219581\n",
            " 15.237511 15.257021 15.278115 15.3008   15.325082 15.350965 15.378456\n",
            " 15.407562 15.43829  15.470645 15.504633 15.540264 15.577541 15.616474\n",
            " 15.657067 15.699327 15.743261 15.788877 15.836179]\n",
            "step 7\n",
            "[15.74629   15.676497  15.684462  15.661432  15.626124  15.589146\n",
            " 15.552687  15.517402  15.483537  15.451197  15.420421  15.391225\n",
            " 15.363606  15.337561  15.313078  15.290143  15.268747  15.248878\n",
            " 15.230523  15.213673  15.198317  15.184447  15.172056  15.161137\n",
            " 15.151684  15.143692  15.137157  15.132078  15.12845   15.126274\n",
            " 15.125549  15.126274  15.128452  15.132084  15.137171  15.143718\n",
            " 15.151727  15.161203  15.172152  15.184576  15.198484  15.2138815\n",
            " 15.230775  15.249171  15.269079  15.290505  15.3134575 15.337947\n",
            " 15.363979  15.391567  15.420716  15.451438  15.48374   15.517633\n",
            " 15.553127  15.590229  15.62895   15.669299  15.711284  15.754912\n",
            " 15.800195 ]\n",
            "step 8\n",
            "[15.714286  15.65309   15.654343  15.630164  15.596315  15.561352\n",
            " 15.527027  15.493869  15.462066  15.431703  15.402807  15.375393\n",
            " 15.349458  15.324992  15.30199   15.280438  15.260327  15.241646\n",
            " 15.224384  15.208534  15.194086  15.181034  15.16937   15.159087\n",
            " 15.150184  15.142654  15.136497  15.131707  15.128286  15.126233\n",
            " 15.125549  15.126234  15.128291  15.131722  15.136533  15.142725\n",
            " 15.150306  15.159279  15.169654  15.181434  15.19463   15.209248\n",
            " 15.225297  15.242786  15.261727  15.28213   15.304002  15.327357\n",
            " 15.352204  15.378557  15.406425  15.435822  15.466756  15.499242\n",
            " 15.53329   15.568912  15.606118  15.644921  15.6853285 15.727356\n",
            " 15.7710085]\n",
            "step 9\n",
            "[15.6892   15.634532 15.630156 15.605053 15.572377 15.539033 15.506423\n",
            " 15.474966 15.444817 15.416038 15.388651 15.362667 15.338079 15.314883\n",
            " 15.29307  15.272627 15.253548 15.235821 15.219439 15.204392 15.190675\n",
            " 15.17828  15.167201 15.157433 15.148972 15.141815 15.135962 15.131408\n",
            " 15.128154 15.126201 15.125549 15.126202 15.12816  15.13143  15.136015\n",
            " 15.14192  15.149152 15.157717 15.167624 15.17888  15.191496 15.20548\n",
            " 15.220842 15.237596 15.255751 15.275319 15.296312 15.318745 15.342629\n",
            " 15.367978 15.394805 15.423124 15.45295  15.484293 15.517168 15.551589\n",
            " 15.587566 15.625117 15.664249 15.704977 15.747312]\n",
            "step 10\n",
            "[15.669392  15.619761  15.610711  15.584864  15.553137  15.521092\n",
            " 15.489861  15.45977   15.430946  15.40344   15.377266  15.3524275\n",
            " 15.328923  15.306746  15.285886  15.266335  15.248085  15.231127\n",
            " 15.215451  15.201053  15.1879225 15.176057  15.165449  15.1560955\n",
            " 15.147993  15.141138  15.135529  15.131165  15.128046  15.126174\n",
            " 15.125549  15.126175  15.128055  15.131192  15.135594  15.141266\n",
            " 15.148215  15.156448  15.165976  15.1768055 15.18895   15.202417\n",
            " 15.217222  15.233376  15.250891  15.26978   15.290059  15.311742\n",
            " 15.334841  15.359375  15.385355  15.4128    15.441721  15.4721365\n",
            " 15.504058  15.537502  15.572483  15.609014  15.647109  15.686781\n",
            " 15.728045 ]\n",
            "step 11\n",
            "[15.653666  15.607972  15.595062  15.568621  15.537657  15.50666\n",
            " 15.476534  15.447543  15.419786  15.393302  15.368099  15.344185\n",
            " 15.32155   15.300191  15.2801    15.261265  15.243682  15.227341\n",
            " 15.212235  15.198358  15.185702  15.174263  15.164036  15.155016\n",
            " 15.147202  15.14059   15.135179  15.130969  15.127959  15.126152\n",
            " 15.125549  15.126153  15.127969  15.131     15.135254  15.140736\n",
            " 15.147454  15.155418  15.164638  15.175121  15.186881  15.199929\n",
            " 15.21428   15.229945  15.24694   15.265278  15.284975  15.306047\n",
            " 15.328509  15.352378  15.377669  15.404401  15.432587  15.462244\n",
            " 15.493389  15.526037  15.5602045 15.595904  15.633153  15.671966\n",
            " 15.712355 ]\n",
            "step 12\n",
            "[15.641132  15.5985365 15.5824585 15.55554   15.525195  15.495044\n",
            " 15.465805  15.437698  15.410798  15.385136  15.360717  15.3375435\n",
            " 15.315609  15.294908  15.275434  15.257177  15.240131  15.224287\n",
            " 15.20964   15.196182  15.183908  15.172813  15.162893  15.154143\n",
            " 15.146562  15.140146  15.134896  15.13081   15.127889  15.126135\n",
            " 15.125549  15.126136  15.127899  15.130844  15.134977  15.1403055\n",
            " 15.146838  15.154583  15.16355   15.173753  15.185202  15.197908\n",
            " 15.211889  15.227158  15.243729  15.261618  15.280841  15.301415\n",
            " 15.323358  15.346686  15.371415  15.397566  15.42515   15.45419\n",
            " 15.484702  15.5167    15.550201  15.585223  15.621779  15.659886\n",
            " 15.697651 ]\n",
            "step 13\n",
            "[15.631112  15.590972  15.572301  15.545002  15.515156  15.485681\n",
            " 15.45716   15.429767  15.403558  15.378555  15.354765  15.332189\n",
            " 15.310819  15.290648  15.271671  15.25388   15.237266  15.221824\n",
            " 15.207545  15.194426  15.18246   15.171643  15.16197   15.153439\n",
            " 15.146045  15.139788  15.134666  15.130681  15.127831  15.126121\n",
            " 15.125549  15.1261215 15.127843  15.130717  15.134753  15.139956\n",
            " 15.1463375 15.153904  15.162668  15.172642  15.183837  15.196267\n",
            " 15.209948  15.224892  15.241119  15.258642  15.27748   15.297649\n",
            " 15.319167  15.342054  15.366324  15.392     15.419096  15.447631\n",
            " 15.477621  15.5090885 15.542046  15.57651   15.612497  15.650028\n",
            " 15.682198 ]\n",
            "step 14\n",
            "[15.623085  15.584897  15.564108  15.536504  15.507061  15.478136\n",
            " 15.4501915 15.423371  15.397717  15.37325   15.349968  15.327871\n",
            " 15.306955  15.287212  15.268636  15.251219  15.234954  15.219833\n",
            " 15.205853  15.193007  15.18129   15.170696  15.161224  15.152867\n",
            " 15.145626  15.139498  15.134481  15.130577  15.127786  15.126109\n",
            " 15.125549  15.12611   15.127797  15.130615  15.134571  15.139674\n",
            " 15.145931  15.153354  15.161953  15.171741  15.18273   15.194935\n",
            " 15.20837   15.223053  15.238998  15.256224  15.274747  15.294587\n",
            " 15.315759  15.3382845 15.362181  15.387467  15.414162  15.442284\n",
            " 15.471849  15.502879  15.535389  15.569396  15.604919  15.639113\n",
            " 15.669783 ]\n",
            "step 15\n",
            "[15.616641  15.580012  15.557499  15.529649  15.500532  15.472048\n",
            " 15.4445715 15.418214  15.393008  15.368969  15.346096  15.324388\n",
            " 15.303838  15.284438  15.266185  15.249069  15.233086  15.218226\n",
            " 15.204487  15.191861  15.180345  15.169931  15.160621  15.152406\n",
            " 15.145288  15.139263  15.134332  15.130493  15.1277485 15.1261\n",
            " 15.125549  15.126101  15.12776   15.130532  15.134424  15.139444\n",
            " 15.145602  15.152907  15.161372  15.171008  15.18183   15.193851\n",
            " 15.207089  15.221559  15.237274  15.254257  15.272525  15.292094\n",
            " 15.312985  15.3352165 15.358808  15.383776  15.410142  15.437923\n",
            " 15.46714   15.497811  15.529954  15.563586  15.598724  15.62881\n",
            " 15.659799 ]\n",
            "step 16\n",
            "[15.611464  15.576082  15.552164  15.524118  15.495264  15.467137\n",
            " 15.440037  15.414051  15.389207  15.365514  15.342972  15.321576\n",
            " 15.301319  15.282199  15.264207  15.247334  15.231577  15.2169285\n",
            " 15.203383  15.190935  15.17958   15.169313  15.160132  15.152033\n",
            " 15.145015  15.139073  15.134211  15.130425  15.127718  15.126092\n",
            " 15.125549  15.126093  15.12773   15.130465  15.134305  15.139258\n",
            " 15.145335  15.152545  15.160901  15.170415  15.181101  15.192973\n",
            " 15.206049  15.220344  15.235875  15.252661  15.270719  15.290069\n",
            " 15.310729  15.33272   15.356058  15.380767  15.406864  15.434369\n",
            " 15.463299  15.493675  15.525516  15.558839  15.592441  15.620529\n",
            " 15.651763 ]\n",
            "step 17\n",
            "[15.607297  15.572918  15.547854  15.519651  15.49101   15.463171\n",
            " 15.436375  15.41069   15.386138  15.362725  15.340448  15.319305\n",
            " 15.299287  15.28039   15.262608  15.245933  15.230359  15.215879\n",
            " 15.202491  15.1901865 15.178963  15.168814  15.159739  15.1517315\n",
            " 15.144793  15.13892   15.134112  15.13037   15.127693  15.126086\n",
            " 15.125549  15.126087  15.127706  15.13041   15.134208  15.139108\n",
            " 15.145119  15.152251  15.160519  15.169933  15.180509  15.192261\n",
            " 15.205204  15.2193575 15.234736  15.251363  15.26925   15.288421\n",
            " 15.308892  15.330687  15.353821  15.378317  15.404192  15.431467\n",
            " 15.460162  15.490298  15.521889  15.554958  15.585196  15.613866\n",
            " 15.64529  ]\n",
            "step 18\n",
            "[15.60394   15.570367  15.544374  15.516046  15.487576  15.45997\n",
            " 15.433419  15.407977  15.38366   15.360473  15.33841   15.3174715\n",
            " 15.297645  15.27893   15.261316  15.2448    15.229374  15.215033\n",
            " 15.20177   15.189581  15.178463  15.168409  15.159419  15.151487\n",
            " 15.144614  15.138796  15.134033  15.130325  15.127674  15.1260805\n",
            " 15.125549  15.126082  15.127686  15.130365  15.13413   15.138986\n",
            " 15.144943  15.152013  15.160209  15.169542  15.180029  15.191682\n",
            " 15.204518  15.218556  15.233814  15.250307  15.268055  15.28708\n",
            " 15.307399  15.329031  15.351997  15.376319  15.402013  15.429103\n",
            " 15.457605  15.487541  15.518928  15.551787  15.579365  15.608502\n",
            " 15.640073 ]\n",
            "step 19\n",
            "[15.601234  15.568308  15.541561  15.51313   15.4848    15.457384\n",
            " 15.431028  15.405786  15.381659  15.358653  15.336764  15.315988\n",
            " 15.296318  15.277749  15.260273  15.243885  15.228578  15.214346\n",
            " 15.201187  15.189092  15.178059  15.168082  15.159162  15.151291\n",
            " 15.144469  15.138695  15.133968  15.130289  15.127658  15.126077\n",
            " 15.125549  15.126079  15.12767   15.13033   15.134066  15.138886\n",
            " 15.1448    15.15182   15.159958  15.169226  15.179639  15.191212\n",
            " 15.203962  15.217906  15.233063  15.249451  15.267085  15.285991\n",
            " 15.306183  15.327685  15.350514  15.374691  15.400238  15.427174\n",
            " 15.455515  15.48529   15.516509  15.549196  15.5746765 15.60418\n",
            " 15.635863 ]\n",
            "step 20\n",
            "[15.599053  15.566647  15.539289  15.510775  15.482559  15.455293\n",
            " 15.429099  15.404013  15.38004   15.3571825 15.335434  15.314791\n",
            " 15.295246  15.276794  15.259429  15.243145  15.227934  15.213792\n",
            " 15.200714  15.188696  15.1777315 15.167818  15.158953  15.15113\n",
            " 15.144352  15.138614  15.133916  15.1302595 15.127645  15.126074\n",
            " 15.125549  15.126075  15.127657  15.1303005 15.134014  15.138805\n",
            " 15.144686  15.151664  15.159754  15.168967  15.179322  15.19083\n",
            " 15.20351   15.21738   15.232453  15.248754  15.266296  15.285104\n",
            " 15.305194  15.326589  15.349305  15.373366  15.39879   15.4256\n",
            " 15.453813  15.483451  15.514531  15.546666  15.570897  15.600697\n",
            " 15.632468 ]\n",
            "step 21\n",
            "[15.597291  15.565303  15.537451  15.508872  15.480745  15.453602\n",
            " 15.427539  15.402581  15.378732  15.355993  15.334358  15.313823\n",
            " 15.294379  15.276023  15.258748  15.242546  15.227413  15.213345\n",
            " 15.200334  15.188375  15.177467  15.167604  15.158783  15.151001\n",
            " 15.144257  15.138548  15.133874  15.130236  15.127634  15.126071\n",
            " 15.125549  15.126073  15.127646  15.130277  15.133972  15.138741\n",
            " 15.144591  15.151537  15.159588  15.168759  15.179066  15.190521\n",
            " 15.203144  15.21695   15.231958  15.248187  15.2656555 15.284383\n",
            " 15.30439   15.325696  15.348321  15.372285  15.397611  15.424316\n",
            " 15.452423  15.481951  15.512918  15.542526  15.567852  15.597886\n",
            " 15.6297245]\n",
            "step 22\n",
            "[15.595871  15.564139  15.535964  15.507331  15.47928   15.452237\n",
            " 15.426277  15.4014225 15.377675  15.355032  15.333489  15.31304\n",
            " 15.293678  15.275398  15.258195  15.242062  15.226993  15.212981\n",
            " 15.200025  15.188117  15.177254  15.167431  15.158647  15.150896\n",
            " 15.14418   15.1384945 15.133841  15.130217  15.127625  15.126069\n",
            " 15.125549  15.12607   15.127638  15.130259  15.133939  15.138688\n",
            " 15.144516  15.151434  15.159454  15.168591  15.178858  15.19027\n",
            " 15.202847  15.216602  15.231557  15.247727  15.265134  15.283797\n",
            " 15.303736  15.32497   15.34752   15.371407  15.396648  15.42327\n",
            " 15.451289  15.480727  15.511601  15.539188  15.565395  15.595619\n",
            " 15.627511 ]\n",
            "step 23\n",
            "[15.594721  15.563194  15.534761  15.506087  15.478096  15.451135\n",
            " 15.425258  15.400488  15.37682   15.354256  15.332785  15.312407\n",
            " 15.293112  15.274894  15.2577505 15.24167   15.226652  15.212688\n",
            " 15.199775  15.187907  15.17708   15.167291  15.158536  15.150811\n",
            " 15.144117  15.138452  15.133813  15.130201  15.127619  15.126067\n",
            " 15.125549  15.126069  15.127631  15.130242  15.133911  15.138644\n",
            " 15.144454  15.15135   15.159346  15.168453  15.178689  15.190066\n",
            " 15.202604  15.216321  15.23123   15.247355  15.264712  15.283322\n",
            " 15.303204  15.324381  15.3468685 15.37069   15.395866  15.422419\n",
            " 15.450365  15.479729  15.510528  15.536497  15.5634165 15.593789\n",
            " 15.625722 ]\n",
            "step 24\n",
            "[15.593793 15.56243  15.533791 15.50508  15.477138 15.450239 15.424435\n",
            " 15.399732 15.376129 15.353627 15.332216 15.311894 15.292652 15.274486\n",
            " 15.257388 15.241353 15.226377 15.212451 15.199573 15.187737 15.17694\n",
            " 15.167177 15.158446 15.150743 15.144067 15.138416 15.13379  15.130189\n",
            " 15.127613 15.126065 15.125549 15.126067 15.127625 15.13023  15.133889\n",
            " 15.13861  15.144404 15.151282 15.159258 15.168343 15.178552 15.189901\n",
            " 15.20241  15.216091 15.230966 15.247052 15.264368 15.282936 15.302772\n",
            " 15.3239   15.346338 15.370108 15.395229 15.421724 15.449613 15.478915\n",
            " 15.50965  15.534327 15.561816 15.592311 15.624275]\n",
            "step 25\n",
            "[15.593041  15.561812  15.533004  15.504267  15.476362  15.449517\n",
            " 15.423766  15.399118  15.375568  15.353119  15.331758  15.3114805\n",
            " 15.292282  15.274156  15.257097  15.241097  15.226153  15.212258\n",
            " 15.1994095 15.1876    15.176827  15.167086  15.158374  15.150687\n",
            " 15.144026  15.138388  15.133772  15.130178  15.127608  15.126064\n",
            " 15.125549  15.126066  15.127621  15.13022   15.133871  15.138582\n",
            " 15.144364  15.151228  15.159186  15.168252  15.178441  15.189768\n",
            " 15.2022505 15.215906  15.230751  15.246806  15.26409   15.282621\n",
            " 15.302421  15.323509  15.345906  15.369634  15.3947115 15.42116\n",
            " 15.448999  15.478251  15.508937  15.532574  15.560527  15.591117\n",
            " 15.623106 ]\n",
            "step 26\n",
            "[15.592436  15.56131   15.532369  15.503608  15.475736  15.448934\n",
            " 15.423227  15.3986225 15.375116  15.352706  15.331384  15.311145\n",
            " 15.291982  15.273888  15.256861  15.2408905 15.225973  15.212103\n",
            " 15.199278  15.187489  15.176735  15.167011  15.158315  15.150642\n",
            " 15.143993  15.138365  15.133758  15.13017   15.1276045 15.126063\n",
            " 15.125549  15.126065  15.127618  15.130212  15.133856  15.138559\n",
            " 15.144332  15.151183  15.159128  15.168179  15.17835   15.18966\n",
            " 15.202122  15.215754  15.2305765 15.246607  15.263863  15.282366\n",
            " 15.302135  15.323193  15.345556  15.369248  15.394289  15.420699\n",
            " 15.448501  15.477713  15.508355  15.531158  15.559485  15.590153\n",
            " 15.62216  ]\n",
            "step 27\n",
            "[15.591947  15.560905  15.531853  15.503075  15.475228  15.44846\n",
            " 15.42279   15.398222  15.374751  15.352374  15.331082  15.310874\n",
            " 15.2917385 15.273671  15.25667   15.240722  15.225827  15.211978\n",
            " 15.199171  15.187399  15.176661  15.166951  15.158267  15.150606\n",
            " 15.143967  15.138347  15.133745  15.130163  15.127602  15.126063\n",
            " 15.125549  15.126064  15.127614  15.130205  15.133844  15.138541\n",
            " 15.144305  15.151148  15.159081  15.168119  15.178278  15.189571\n",
            " 15.202017  15.215632  15.230434  15.246445  15.263679  15.282159\n",
            " 15.301903  15.322935  15.34527   15.368935  15.393946  15.420326\n",
            " 15.448092  15.477271  15.507881  15.530017  15.558642  15.589372\n",
            " 15.621396 ]\n",
            "step 28\n",
            "[15.591549  15.560577  15.531435  15.502643  15.474818  15.448078\n",
            " 15.422436  15.397897  15.3744545 15.352104  15.330839  15.310654\n",
            " 15.291542  15.273498  15.256515  15.240586  15.22571   15.211876\n",
            " 15.199083  15.187326  15.1766    15.166903  15.158229  15.150577\n",
            " 15.143946  15.138331  15.133736  15.130158  15.1276    15.126062\n",
            " 15.125549  15.126063  15.127612  15.130199  15.133835  15.138526\n",
            " 15.144283  15.151118  15.159043  15.168072  15.178219  15.1895\n",
            " 15.201933  15.215533  15.230321  15.246313  15.263529  15.281991\n",
            " 15.301716  15.322725  15.345039  15.36868   15.393665  15.420019\n",
            " 15.447762  15.476914  15.507494  15.529094  15.557962  15.588742\n",
            " 15.620777 ]\n",
            "step 29\n",
            "[15.591229  15.560311  15.531098  15.502294  15.474485  15.447768\n",
            " 15.422151  15.397635  15.374215  15.351886  15.330643  15.310475\n",
            " 15.291383  15.2733555 15.25639   15.240477  15.225614  15.211794\n",
            " 15.199014  15.187267  15.176552  15.1668625 15.158197  15.150553\n",
            " 15.143928  15.138319  15.133728  15.130154  15.127598  15.126061\n",
            " 15.125549  15.126063  15.12761   15.130196  15.133827  15.138514\n",
            " 15.144266  15.151094  15.159012  15.168033  15.178171  15.189443\n",
            " 15.201864  15.215452  15.230227  15.246206  15.263408  15.281854\n",
            " 15.301562  15.322556  15.344851  15.368472  15.393438  15.419773\n",
            " 15.447494  15.476623  15.507179  15.528347  15.557412  15.588231\n",
            " 15.620277 ]\n",
            "step 30\n",
            "[15.590968  15.560095  15.530827  15.502012  15.474216  15.447516\n",
            " 15.421919  15.397423  15.374021  15.351709  15.3304825 15.310333\n",
            " 15.291254  15.273241  15.256289  15.240388  15.225536  15.211727\n",
            " 15.1989565 15.18722   15.176513  15.166831  15.158173  15.150534\n",
            " 15.143913  15.1383095 15.133721  15.13015   15.127596  15.126061\n",
            " 15.125549  15.126063  15.127608  15.130192  15.1338215 15.138504\n",
            " 15.144252  15.151075  15.158987  15.168001  15.178132  15.189395\n",
            " 15.201808  15.215387  15.230151  15.2461195 15.26331   15.281743\n",
            " 15.301438  15.322417  15.344698  15.368304  15.393253  15.419571\n",
            " 15.447274  15.476384  15.506925  15.527744  15.556968  15.587819\n",
            " 15.619872 ]\n",
            "step 31\n",
            "[15.590758  15.559919  15.530604  15.501781  15.473999  15.447314\n",
            " 15.42173   15.39725   15.373863  15.351567  15.330354  15.310216\n",
            " 15.29115   15.273149  15.256206  15.2403145 15.225473  15.211673\n",
            " 15.198911  15.187181  15.176481  15.166805  15.158152  15.150518\n",
            " 15.143902  15.138301  15.133717  15.130147  15.127595  15.126061\n",
            " 15.125549  15.126062  15.127607  15.130189  15.133816  15.138496\n",
            " 15.14424   15.15106   15.158967  15.167975  15.178101  15.189358\n",
            " 15.201763  15.215334  15.23009   15.24605   15.26323   15.281653\n",
            " 15.301337  15.322305  15.344573  15.368168  15.393104  15.419408\n",
            " 15.447095  15.476192  15.506716  15.527257  15.556608  15.587485\n",
            " 15.619543 ]\n",
            "step 32\n",
            "[15.590589  15.55978   15.530425  15.5015955 15.473823  15.44715\n",
            " 15.42158   15.39711   15.373736  15.351452  15.330249  15.3101225\n",
            " 15.291066  15.273073  15.256139  15.240257  15.225423  15.211629\n",
            " 15.1988735 15.18715   15.176455  15.166784  15.158135  15.150506\n",
            " 15.143892  15.138295  15.133713  15.130145  15.127594  15.1260605\n",
            " 15.125549  15.126062  15.127606  15.130187  15.133812  15.13849\n",
            " 15.144232  15.151048  15.158951  15.167954  15.178075  15.189326\n",
            " 15.201727  15.215292  15.230041  15.245993  15.263165  15.28158\n",
            " 15.301255  15.322214  15.344472  15.368055  15.3929825 15.419275\n",
            " 15.446952  15.476035  15.506547  15.526863  15.556317  15.587215\n",
            " 15.619279 ]\n",
            "step 33\n",
            "[15.590452  15.559666  15.530281  15.501448  15.47368   15.447017\n",
            " 15.421456  15.396997  15.373633  15.351357  15.330164  15.310045\n",
            " 15.2909975 15.273012  15.256084  15.24021   15.225382  15.211594\n",
            " 15.198843  15.187124  15.176434  15.166767  15.158122  15.150495\n",
            " 15.143885  15.138289  15.133709  15.130143  15.127593  15.1260605\n",
            " 15.125549  15.126062  15.127605  15.130185  15.133808  15.138485\n",
            " 15.144224  15.151037  15.158937  15.167938  15.178055  15.1893015\n",
            " 15.201697  15.215257  15.2300005 15.245946  15.263113  15.281521\n",
            " 15.301188  15.322141  15.344392  15.367967  15.392883  15.419166\n",
            " 15.446834  15.475908  15.506409  15.526545  15.556081  15.586998\n",
            " 15.619064 ]\n",
            "step 34\n",
            "[15.59034   15.559573  15.530163  15.501326  15.473565  15.446908\n",
            " 15.421358  15.396908  15.373549  15.351283  15.330095  15.309984\n",
            " 15.290941  15.272963  15.256042  15.240171  15.225348  15.211565\n",
            " 15.198819  15.187104  15.176417  15.166754  15.158112  15.150487\n",
            " 15.143879  15.138286  15.133706  15.130141  15.127592  15.1260605\n",
            " 15.125549  15.126062  15.1276045 15.130183  15.133805  15.13848\n",
            " 15.1442175 15.151029  15.158927  15.167925  15.178038  15.189281\n",
            " 15.201673  15.215228  15.229967  15.245909  15.26307   15.281473\n",
            " 15.301135  15.32208   15.344324  15.367891  15.392802  15.419079\n",
            " 15.446737  15.475806  15.506298  15.526286  15.555894  15.586822\n",
            " 15.61889  ]\n",
            "step 35\n",
            "[15.590249  15.559498  15.530069  15.501227  15.473471  15.446822\n",
            " 15.421276  15.396833  15.373483  15.35122   15.330039  15.309935\n",
            " 15.290897  15.2729225 15.256006  15.240141  15.225322  15.211541\n",
            " 15.198799  15.187088  15.176404  15.166742  15.158103  15.15048\n",
            " 15.143874  15.138282  15.133704  15.13014   15.127592  15.1260605\n",
            " 15.125549  15.126061  15.1276045 15.130182  15.133803  15.138477\n",
            " 15.144213  15.151022  15.158918  15.167913  15.178024  15.189264\n",
            " 15.2016535 15.215206  15.229941  15.245879  15.263036  15.281435\n",
            " 15.301092  15.322031  15.344271  15.367833  15.392737  15.419007\n",
            " 15.446661  15.47572   15.506207  15.526078  15.555738  15.586678\n",
            " 15.618751 ]\n",
            "step 36\n",
            "[15.590178  15.559436  15.529993  15.501149  15.473394  15.446752\n",
            " 15.421211  15.396774  15.373428  15.3511715 15.329995  15.309895\n",
            " 15.290861  15.272891  15.255978  15.240115  15.225299  15.211523\n",
            " 15.198783  15.187074  15.176393  15.166734  15.158095  15.150475\n",
            " 15.143869  15.138279  15.133702  15.130139  15.127591  15.1260605\n",
            " 15.125549  15.126061  15.127604  15.130181  15.133801  15.138474\n",
            " 15.144209  15.151016  15.158912  15.167905  15.178013  15.189251\n",
            " 15.201637  15.215187  15.22992   15.245854  15.263007  15.281402\n",
            " 15.301056  15.321992  15.344226  15.3677845 15.392685  15.418949\n",
            " 15.446597  15.475653  15.506133  15.525909  15.555614  15.586563\n",
            " 15.618636 ]\n",
            "step 37\n",
            "[15.5901165 15.559387  15.529929  15.501084  15.473334  15.446694\n",
            " 15.421159  15.396726  15.373384  15.351132  15.329959  15.30986\n",
            " 15.290832  15.272864  15.255955  15.240095  15.225282  15.211508\n",
            " 15.1987705 15.187063  15.176383  15.166726  15.15809   15.15047\n",
            " 15.143867  15.138277  15.1337    15.130138  15.127591  15.1260605\n",
            " 15.125549  15.126061  15.127604  15.13018   15.1338005 15.138472\n",
            " 15.144205  15.151012  15.158905  15.167898  15.178004  15.18924\n",
            " 15.201625  15.215174  15.229903  15.245834  15.262984  15.281377\n",
            " 15.301027  15.321959  15.344192  15.367745  15.392642  15.418902\n",
            " 15.446546  15.475597  15.506073  15.525772  15.555511  15.586469\n",
            " 15.618545 ]\n",
            "step 38\n",
            "[15.59007   15.559347  15.529879  15.501032  15.473284  15.446649\n",
            " 15.421117  15.396686  15.373349  15.351099  15.329929  15.3098345\n",
            " 15.290808  15.272843  15.255937  15.240079  15.225267  15.211495\n",
            " 15.198759  15.187055  15.176376  15.16672   15.158085  15.150467\n",
            " 15.143864  15.138275  15.133699  15.130137  15.12759   15.12606\n",
            " 15.125549  15.126061  15.127603  15.130179  15.133799  15.13847\n",
            " 15.144203  15.151009  15.1589    15.1678915 15.177998  15.189232\n",
            " 15.201614  15.215161  15.229889  15.245818  15.262966  15.281357\n",
            " 15.301004  15.321934  15.344163  15.367715  15.392607  15.418863\n",
            " 15.446505  15.475553  15.506024  15.5256605 15.555432  15.586392\n",
            " 15.618468 ]\n",
            "step 39\n",
            "[15.590031  15.559315  15.529838  15.500989  15.473244  15.44661\n",
            " 15.421082  15.396655  15.37332   15.351072  15.329905  15.3098135\n",
            " 15.29079   15.272826  15.255921  15.240065  15.225256  15.211486\n",
            " 15.198751  15.187047  15.17637   15.166716  15.158081  15.150463\n",
            " 15.143862  15.138273  15.133698  15.130137  15.12759   15.12606\n",
            " 15.125549  15.126061  15.127603  15.130179  15.133798  15.138469\n",
            " 15.144201  15.151006  15.158897  15.167887  15.177991  15.189225\n",
            " 15.201606  15.215151  15.229877  15.245805  15.262952  15.28134\n",
            " 15.300985  15.321913  15.344139  15.367688  15.392578  15.418833\n",
            " 15.446472  15.475514  15.505985  15.525573  15.5553665 15.58633\n",
            " 15.618408 ]\n",
            "step 40\n",
            "[15.59      15.559289  15.529805  15.500955  15.473212  15.446581\n",
            " 15.421055  15.396629  15.373296  15.351051  15.329886  15.309796\n",
            " 15.290772  15.272812  15.255909  15.240054  15.225245  15.211477\n",
            " 15.198745  15.187041  15.176365  15.166712  15.158078  15.150461\n",
            " 15.14386   15.138272  15.133698  15.1301365 15.12759   15.12606\n",
            " 15.125549  15.126061  15.127603  15.130178  15.133798  15.138467\n",
            " 15.144199  15.151004  15.158894  15.167883  15.177986  15.189219\n",
            " 15.201599  15.215141  15.229868  15.245794  15.262939  15.281326\n",
            " 15.30097   15.321896  15.344119  15.367668  15.392555  15.418808\n",
            " 15.4464445 15.475487  15.505955  15.525499  15.555311  15.586281\n",
            " 15.61836  ]\n",
            "step 41\n",
            "[15.589976  15.559269  15.5297785 15.500928  15.473186  15.446556\n",
            " 15.421031  15.396606  15.373277  15.351034  15.32987   15.309781\n",
            " 15.290761  15.272801  15.255899  15.240046  15.22524   15.211471\n",
            " 15.198739  15.1870365 15.176361  15.166708  15.158076  15.150459\n",
            " 15.143859  15.138271  15.1336975 15.1301365 15.12759   15.12606\n",
            " 15.125549  15.126061  15.127603  15.130178  15.133797  15.138466\n",
            " 15.144197  15.151002  15.158892  15.16788   15.177982  15.189215\n",
            " 15.201594  15.215137  15.22986   15.245786  15.262929  15.281315\n",
            " 15.300958  15.321882  15.344105  15.367651  15.392537  15.418788\n",
            " 15.446422  15.475461  15.505929  15.525441  15.555268  15.586243\n",
            " 15.61832  ]\n",
            "step 42\n",
            "[15.589954  15.55925   15.529757  15.500905  15.473163  15.446537\n",
            " 15.421013  15.39659   15.373261  15.35102   15.329857  15.309771\n",
            " 15.2907505 15.272792  15.255891  15.24004   15.225233  15.211466\n",
            " 15.198734  15.187033  15.176358  15.166706  15.158073  15.150458\n",
            " 15.143858  15.13827   15.133697  15.1301365 15.12759   15.12606\n",
            " 15.125549  15.126061  15.127603  15.130178  15.133796  15.138466\n",
            " 15.1441965 15.151     15.15889   15.167877  15.177979  15.189211\n",
            " 15.20159   15.215132  15.229855  15.245779  15.262921  15.281305\n",
            " 15.300947  15.321871  15.344091  15.367637  15.392523  15.418771\n",
            " 15.4464035 15.475444  15.505907  15.525393  15.555234  15.58621\n",
            " 15.618288 ]\n",
            "step 43\n",
            "[15.589935  15.5592375 15.529739  15.500886  15.473146  15.446521\n",
            " 15.420999  15.396578  15.373249  15.351008  15.329847  15.309761\n",
            " 15.290742  15.272784  15.255885  15.240033  15.225227  15.211461\n",
            " 15.198731  15.18703   15.176355  15.166704  15.158072  15.150456\n",
            " 15.143857  15.13827   15.133697  15.1301365 15.12759   15.12606\n",
            " 15.125549  15.126061  15.127603  15.1301775 15.133796  15.138465\n",
            " 15.144196  15.150999  15.158888  15.167875  15.177977  15.189207\n",
            " 15.201585  15.215126  15.229849  15.245772  15.262916  15.281298\n",
            " 15.30094   15.32186   15.344083  15.367625  15.39251   15.418757\n",
            " 15.446388  15.475428  15.50589   15.525355  15.555205  15.586184\n",
            " 15.618261 ]\n",
            "step 44\n",
            "[15.589922  15.559227  15.529725  15.500871  15.473132  15.4465065\n",
            " 15.420986  15.396566  15.373239  15.350999  15.32984   15.309753\n",
            " 15.290735  15.2727785 15.255879  15.240028  15.225224  15.211458\n",
            " 15.198728  15.187028  15.176354  15.166702  15.158071  15.150455\n",
            " 15.143856  15.138269  15.133697  15.130136  15.12759   15.12606\n",
            " 15.125549  15.126061  15.127603  15.1301775 15.133796  15.138464\n",
            " 15.144195  15.150998  15.158887  15.167874  15.177975  15.189206\n",
            " 15.201582  15.215124  15.229846  15.245769  15.262911  15.281292\n",
            " 15.300933  15.321855  15.344074  15.367617  15.392501  15.418748\n",
            " 15.446378  15.475413  15.505876  15.525324  15.555182  15.586161\n",
            " 15.618241 ]\n",
            "step 45\n",
            "[15.589913  15.559217  15.529713  15.50086   15.473122  15.446496\n",
            " 15.420976  15.396558  15.37323   15.350991  15.329833  15.309747\n",
            " 15.29073   15.272775  15.255874  15.240025  15.22522   15.211455\n",
            " 15.198726  15.187025  15.176352  15.166701  15.15807   15.1504545\n",
            " 15.143855  15.138269  15.133696  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.1301775 15.133796  15.138464\n",
            " 15.144195  15.150997  15.158886  15.167872  15.177974  15.189204\n",
            " 15.20158   15.215121  15.229842  15.245766  15.262906  15.281287\n",
            " 15.300928  15.321848  15.344068  15.36761   15.392491  15.418737\n",
            " 15.446368  15.475403  15.505866  15.525299  15.555162  15.586144\n",
            " 15.618224 ]\n",
            "step 46\n",
            "[15.589903  15.559208  15.529704  15.50085   15.473112  15.446487\n",
            " 15.420968  15.39655   15.373224  15.350986  15.329826  15.309743\n",
            " 15.290726  15.272771  15.255872  15.240022  15.225218  15.2114525\n",
            " 15.198724  15.187023  15.176351  15.166699  15.158069  15.1504545\n",
            " 15.143855  15.138268  15.133696  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.1301775 15.133795  15.138464\n",
            " 15.144194  15.150996  15.158885  15.167871  15.177972  15.189201\n",
            " 15.201578  15.215118  15.229839  15.245761  15.262902  15.281284\n",
            " 15.300923  15.321843  15.344062  15.367603  15.392485  15.418732\n",
            " 15.446361  15.475395  15.505855  15.525279  15.555149  15.58613\n",
            " 15.618211 ]\n",
            "step 47\n",
            "[15.589897  15.559203  15.529697  15.500843  15.4731045 15.446482\n",
            " 15.420962  15.396544  15.373218  15.350981  15.3298235 15.309739\n",
            " 15.290722  15.272767  15.255868  15.240019  15.225216  15.211451\n",
            " 15.198722  15.187022  15.176349  15.166699  15.158068  15.150454\n",
            " 15.143854  15.138268  15.133696  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.1301775 15.133795  15.138463\n",
            " 15.144194  15.150996  15.158884  15.1678705 15.177971  15.1892\n",
            " 15.201577  15.2151165 15.229837  15.24576   15.262899  15.2812805\n",
            " 15.30092   15.321838  15.344058  15.367598  15.39248   15.418725\n",
            " 15.446353  15.475389  15.505848  15.525262  15.555137  15.586118\n",
            " 15.618198 ]\n",
            "step 48\n",
            "[15.589891  15.559198  15.52969   15.500835  15.473099  15.446476\n",
            " 15.420958  15.396541  15.373214  15.350977  15.329819  15.309735\n",
            " 15.29072   15.272763  15.255866  15.240018  15.225214  15.21145\n",
            " 15.198721  15.187021  15.176349  15.166698  15.158067  15.150454\n",
            " 15.143854  15.138268  15.133696  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.1301775 15.133795  15.138463\n",
            " 15.144194  15.150995  15.158883  15.16787   15.17797   15.189199\n",
            " 15.201576  15.215115  15.2298355 15.245757  15.2628975 15.281278\n",
            " 15.300918  15.321836  15.344055  15.367595  15.392476  15.418721\n",
            " 15.446348  15.475383  15.505841  15.525248  15.555124  15.586108\n",
            " 15.618188 ]\n",
            "step 49\n",
            "[15.589886  15.559194  15.529685  15.500832  15.473094  15.446471\n",
            " 15.420953  15.396536  15.373211  15.350974  15.329816  15.3097315\n",
            " 15.290717  15.272762  15.255864  15.240015  15.225212  15.211449\n",
            " 15.19872   15.18702   15.176348  15.1666975 15.158068  15.150453\n",
            " 15.143854  15.138268  15.133696  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.1301775 15.133795  15.138463\n",
            " 15.144193  15.150995  15.158883  15.167869  15.177969  15.1891985\n",
            " 15.201575  15.215114  15.229835  15.245755  15.262897  15.281276\n",
            " 15.300915  15.321835  15.344051  15.367591  15.392472  15.418717\n",
            " 15.446345  15.475379  15.505837  15.525239  15.555117  15.5861025\n",
            " 15.618183 ]\n",
            "step 50\n",
            "[15.589882  15.559191  15.52968   15.500827  15.473089  15.446467\n",
            " 15.420949  15.396532  15.373208  15.350972  15.329814  15.30973\n",
            " 15.290714  15.27276   15.255862  15.240014  15.225212  15.211448\n",
            " 15.198719  15.187019  15.176348  15.166697  15.158067  15.150453\n",
            " 15.143853  15.138268  15.133695  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133795  15.138463\n",
            " 15.144193  15.150994  15.158882  15.167869  15.177969  15.189198\n",
            " 15.201574  15.215113  15.229834  15.245754  15.262896  15.281275\n",
            " 15.300914  15.321831  15.3440485 15.367588  15.392469  15.418714\n",
            " 15.446341  15.475373  15.505834  15.525229  15.555113  15.586096\n",
            " 15.618176 ]\n",
            "step 51\n",
            "[15.589879  15.559189  15.529678  15.500824  15.473086  15.446464\n",
            " 15.420946  15.39653   15.373204  15.350968  15.329813  15.309729\n",
            " 15.290713  15.272759  15.255862  15.240013  15.225211  15.211447\n",
            " 15.198718  15.187019  15.176347  15.166697  15.158067  15.150452\n",
            " 15.143853  15.1382675 15.133696  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138463\n",
            " 15.144193  15.150994  15.158882  15.167869  15.177968  15.189197\n",
            " 15.201572  15.215113  15.229832  15.245752  15.262894  15.281274\n",
            " 15.300911  15.321829  15.344047  15.367587  15.392468  15.418712\n",
            " 15.446339  15.475371  15.505831  15.525221  15.555106  15.586091\n",
            " 15.618171 ]\n",
            "step 52\n",
            "[15.589878  15.559187  15.529675  15.500821  15.4730835 15.446462\n",
            " 15.420945  15.396527  15.373203  15.350966  15.32981   15.309727\n",
            " 15.290712  15.2727585 15.255861  15.240012  15.225209  15.211446\n",
            " 15.198717  15.187018  15.176346  15.166696  15.158067  15.150452\n",
            " 15.143853  15.1382675 15.133696  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138463\n",
            " 15.144193  15.150994  15.158882  15.167868  15.177968  15.189198\n",
            " 15.201571  15.215111  15.229831  15.245753  15.262893  15.281273\n",
            " 15.300911  15.321828  15.344046  15.367585  15.392466  15.41871\n",
            " 15.446336  15.47537   15.505827  15.525216  15.555102  15.586087\n",
            " 15.618167 ]\n",
            "step 53\n",
            "[15.589875  15.559185  15.529675  15.500817  15.473082  15.446459\n",
            " 15.420942  15.396526  15.373202  15.3509655 15.329808  15.309727\n",
            " 15.290712  15.272758  15.255859  15.240012  15.225209  15.211446\n",
            " 15.198717  15.187018  15.176346  15.166697  15.158066  15.150453\n",
            " 15.143853  15.1382675 15.133695  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144193  15.150994  15.158882  15.167868  15.177967  15.189197\n",
            " 15.201572  15.215111  15.229831  15.245752  15.262892  15.281271\n",
            " 15.300909  15.321827  15.344044  15.367584  15.392465  15.418708\n",
            " 15.446335  15.4753685 15.505825  15.525212  15.555098  15.586083\n",
            " 15.618164 ]\n",
            "step 54\n",
            "[15.589872  15.559183  15.529672  15.500816  15.47308   15.446458\n",
            " 15.42094   15.396524  15.373201  15.3509655 15.329808  15.309726\n",
            " 15.29071   15.272757  15.255859  15.240011  15.225209  15.211445\n",
            " 15.198717  15.187018  15.176346  15.166696  15.158066  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133795  15.138462\n",
            " 15.144193  15.150994  15.158882  15.167868  15.177967  15.189197\n",
            " 15.201571  15.215111  15.229831  15.24575   15.262891  15.281271\n",
            " 15.300909  15.321827  15.344043  15.367582  15.392463  15.418706\n",
            " 15.446333  15.475367  15.505822  15.5252075 15.555097  15.586081\n",
            " 15.618162 ]\n",
            "step 55\n",
            "[15.58987   15.559181  15.529672  15.500815  15.47308   15.446456\n",
            " 15.4209385 15.396524  15.3732    15.350965  15.329806  15.309724\n",
            " 15.2907095 15.272756  15.255858  15.24001   15.225208  15.211445\n",
            " 15.198716  15.187017  15.176346  15.166696  15.158066  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133795  15.138463\n",
            " 15.144192  15.150994  15.158881  15.167867  15.177967  15.189196\n",
            " 15.201571  15.215109  15.22983   15.24575   15.262891  15.28127\n",
            " 15.300907  15.321825  15.344043  15.367582  15.392461  15.418706\n",
            " 15.446332  15.475365  15.505822  15.525205  15.555095  15.586079\n",
            " 15.618158 ]\n",
            "step 56\n",
            "[15.58987   15.559181  15.52967   15.500815  15.473077  15.446456\n",
            " 15.420938  15.3965225 15.3731985 15.350964  15.329806  15.309723\n",
            " 15.290709  15.272755  15.255858  15.24001   15.225208  15.211445\n",
            " 15.198716  15.187017  15.176346  15.166696  15.158066  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138463\n",
            " 15.144192  15.150994  15.158881  15.167868  15.177967  15.189196\n",
            " 15.2015705 15.215109  15.22983   15.24575   15.26289   15.281269\n",
            " 15.300908  15.321824  15.344042  15.367581  15.392461  15.418704\n",
            " 15.446331  15.475365  15.505821  15.525202  15.555091  15.586079\n",
            " 15.618159 ]\n",
            "step 57\n",
            "[15.58987   15.559182  15.529669  15.5008135 15.473077  15.446454\n",
            " 15.4209385 15.396522  15.373198  15.350962  15.329805  15.309725\n",
            " 15.290709  15.272755  15.255857  15.24001   15.225208  15.211444\n",
            " 15.198716  15.1870165 15.176346  15.166696  15.158066  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150994  15.158881  15.167868  15.177967  15.189196\n",
            " 15.2015705 15.215109  15.22983   15.24575   15.26289   15.281269\n",
            " 15.300906  15.321825  15.344041  15.367581  15.392461  15.418704\n",
            " 15.446331  15.475363  15.50582   15.525201  15.555091  15.586077\n",
            " 15.618156 ]\n",
            "step 58\n",
            "[15.5898695 15.55918   15.529667  15.500813  15.473076  15.446454\n",
            " 15.420938  15.396522  15.3731985 15.350963  15.329804  15.309723\n",
            " 15.290709  15.272755  15.255857  15.240009  15.225208  15.211444\n",
            " 15.198715  15.187017  15.176345  15.166696  15.158066  15.150452\n",
            " 15.143852  15.1382675 15.133695  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150994  15.158881  15.167867  15.177966  15.189196\n",
            " 15.2015705 15.215109  15.229829  15.245749  15.26289   15.281269\n",
            " 15.300906  15.321824  15.344042  15.367579  15.39246   15.418704\n",
            " 15.446329  15.475363  15.505818  15.525199  15.555088  15.586075\n",
            " 15.618156 ]\n",
            "step 59\n",
            "[15.589869  15.559178  15.529666  15.500812  15.473075  15.446454\n",
            " 15.420936  15.396521  15.373198  15.350961  15.329805  15.309724\n",
            " 15.290709  15.272755  15.2558565 15.240009  15.225207  15.211444\n",
            " 15.198716  15.187017  15.176346  15.166696  15.158065  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177967  15.189196\n",
            " 15.2015705 15.215109  15.229829  15.245749  15.262889  15.281269\n",
            " 15.300905  15.321824  15.344041  15.36758   15.392459  15.418703\n",
            " 15.44633   15.475362  15.505818  15.525197  15.555088  15.586074\n",
            " 15.6181555]\n",
            "step 60\n",
            "[15.589869  15.55918   15.529665  15.500812  15.473075  15.446453\n",
            " 15.420938  15.396521  15.373197  15.350962  15.329805  15.309723\n",
            " 15.290709  15.272754  15.255857  15.240009  15.225207  15.211444\n",
            " 15.198716  15.1870165 15.176345  15.166695  15.158066  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150994  15.158881  15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215108  15.229829  15.24575   15.262888  15.281267\n",
            " 15.300905  15.321823  15.34404   15.3675785 15.392459  15.418702\n",
            " 15.446328  15.475362  15.505818  15.525196  15.555088  15.586075\n",
            " 15.618154 ]\n",
            "step 61\n",
            "[15.589868  15.559179  15.529665  15.500811  15.473074  15.446453\n",
            " 15.420936  15.396521  15.373198  15.350961  15.329804  15.309723\n",
            " 15.290708  15.272754  15.255857  15.240008  15.225207  15.211444\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158065  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189196\n",
            " 15.2015705 15.215109  15.229829  15.245749  15.262889  15.281268\n",
            " 15.300905  15.321824  15.344041  15.3675785 15.392459  15.418701\n",
            " 15.446329  15.475361  15.505818  15.525195  15.555087  15.586073\n",
            " 15.618154 ]\n",
            "step 62\n",
            "[15.589869  15.559179  15.529665  15.500811  15.473075  15.446453\n",
            " 15.420936  15.396521  15.373196  15.350962  15.329803  15.309722\n",
            " 15.290707  15.272754  15.2558565 15.240009  15.225206  15.211444\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158065  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189196\n",
            " 15.2015705 15.215108  15.229829  15.245749  15.262889  15.281267\n",
            " 15.300905  15.321823  15.34404   15.3675785 15.392458  15.418702\n",
            " 15.446327  15.475361  15.505818  15.525196  15.555086  15.586072\n",
            " 15.618154 ]\n",
            "step 63\n",
            "[15.589866  15.559178  15.529667  15.50081   15.473074  15.446452\n",
            " 15.420935  15.396521  15.373197  15.35096   15.329804  15.309721\n",
            " 15.290708  15.272754  15.2558565 15.240009  15.225207  15.211443\n",
            " 15.198715  15.187017  15.176345  15.166696  15.158065  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177967  15.189196\n",
            " 15.2015705 15.215109  15.229829  15.245749  15.262889  15.281267\n",
            " 15.300905  15.321822  15.34404   15.3675785 15.392459  15.418701\n",
            " 15.446328  15.475359  15.505817  15.525194  15.555086  15.586072\n",
            " 15.618153 ]\n",
            "step 64\n",
            "[15.589867  15.559178  15.529665  15.500811  15.473074  15.446451\n",
            " 15.420937  15.39652   15.373197  15.350962  15.329803  15.309722\n",
            " 15.290708  15.272754  15.255857  15.240009  15.225207  15.211444\n",
            " 15.198715  15.187017  15.176345  15.166696  15.158066  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177967  15.189195\n",
            " 15.2015705 15.215108  15.229829  15.2457485 15.262889  15.281267\n",
            " 15.300906  15.321823  15.344041  15.3675785 15.392458  15.418702\n",
            " 15.446327  15.475361  15.505818  15.525193  15.555085  15.586072\n",
            " 15.618152 ]\n",
            "step 65\n",
            "[15.589867  15.559178  15.529665  15.50081   15.473074  15.446452\n",
            " 15.420935  15.396519  15.373197  15.350961  15.329803  15.309722\n",
            " 15.290707  15.272754  15.2558565 15.240008  15.225207  15.211443\n",
            " 15.198716  15.1870165 15.176345  15.166696  15.158065  15.150451\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177967  15.189195\n",
            " 15.2015705 15.215109  15.229829  15.2457485 15.262889  15.281268\n",
            " 15.300905  15.321822  15.34404   15.3675785 15.392458  15.418702\n",
            " 15.446328  15.475361  15.505816  15.525192  15.555084  15.586071\n",
            " 15.618152 ]\n",
            "step 66\n",
            "[15.589866  15.559176  15.529665  15.500811  15.473074  15.446451\n",
            " 15.420935  15.396521  15.373196  15.35096   15.329803  15.309723\n",
            " 15.290707  15.272754  15.2558565 15.240008  15.225206  15.211444\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158066  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133795  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215108  15.229828  15.245749  15.262888  15.281267\n",
            " 15.300905  15.321823  15.344039  15.3675785 15.392457  15.418701\n",
            " 15.446328  15.475359  15.505817  15.525194  15.555084  15.586072\n",
            " 15.618152 ]\n",
            "step 67\n",
            "[15.589867  15.559177  15.529665  15.500809  15.473074  15.446452\n",
            " 15.420935  15.396521  15.373197  15.35096   15.329804  15.309723\n",
            " 15.290707  15.272753  15.2558565 15.240009  15.225207  15.211443\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158065  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189196\n",
            " 15.2015705 15.215109  15.229829  15.245749  15.262889  15.281267\n",
            " 15.300905  15.321823  15.344039  15.3675785 15.392458  15.418701\n",
            " 15.446328  15.47536   15.505817  15.525192  15.555084  15.586072\n",
            " 15.618152 ]\n",
            "step 68\n",
            "[15.589868  15.559178  15.529665  15.50081   15.473072  15.446451\n",
            " 15.420935  15.396519  15.373196  15.350961  15.329804  15.309722\n",
            " 15.290708  15.272753  15.2558565 15.240009  15.225206  15.211443\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158065  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215108  15.229829  15.2457485 15.262888  15.281267\n",
            " 15.300905  15.321822  15.344039  15.3675785 15.392459  15.4187\n",
            " 15.446326  15.475359  15.505817  15.525192  15.555083  15.586071\n",
            " 15.618152 ]\n",
            "step 69\n",
            "[15.589868  15.559178  15.529666  15.50081   15.473073  15.446452\n",
            " 15.420935  15.396519  15.373196  15.350961  15.3298025 15.309722\n",
            " 15.290707  15.272753  15.2558565 15.240008  15.225206  15.211443\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158065  15.150452\n",
            " 15.143852  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215109  15.229829  15.245749  15.262888  15.281268\n",
            " 15.300905  15.321822  15.344039  15.3675785 15.392457  15.4187\n",
            " 15.446326  15.475361  15.505816  15.525193  15.555082  15.586071\n",
            " 15.618152 ]\n",
            "step 70\n",
            "[15.589865  15.559177  15.529665  15.500811  15.473073  15.446451\n",
            " 15.420935  15.39652   15.373196  15.350961  15.329803  15.309721\n",
            " 15.290706  15.272754  15.2558565 15.240009  15.225207  15.211443\n",
            " 15.198715  15.187017  15.176345  15.166696  15.158065  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215109  15.229829  15.245749  15.262888  15.281267\n",
            " 15.300904  15.321822  15.344039  15.367578  15.392457  15.418702\n",
            " 15.446327  15.47536   15.505816  15.525191  15.555084  15.586072\n",
            " 15.618151 ]\n",
            "step 71\n",
            "[15.589866  15.559178  15.529664  15.500809  15.473074  15.446451\n",
            " 15.420935  15.39652   15.373197  15.35096   15.329803  15.309721\n",
            " 15.290707  15.272754  15.2558565 15.240008  15.225206  15.211443\n",
            " 15.198715  15.1870165 15.176345  15.166696  15.158065  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177967  15.189195\n",
            " 15.20157   15.215109  15.229829  15.2457485 15.262888  15.281267\n",
            " 15.300904  15.321823  15.344039  15.3675785 15.392458  15.418701\n",
            " 15.446326  15.475359  15.505816  15.525191  15.555084  15.586071\n",
            " 15.618153 ]\n",
            "step 72\n",
            "[15.589867  15.559178  15.529664  15.50081   15.473074  15.446452\n",
            " 15.420936  15.39652   15.373196  15.35096   15.329803  15.309721\n",
            " 15.290707  15.272754  15.2558565 15.240008  15.225207  15.211443\n",
            " 15.198715  15.187017  15.176345  15.166696  15.158065  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215109  15.229828  15.2457485 15.262888  15.281267\n",
            " 15.300904  15.321823  15.344039  15.3675785 15.392457  15.4187\n",
            " 15.446328  15.475359  15.505816  15.525193  15.555086  15.586071\n",
            " 15.618153 ]\n",
            "step 73\n",
            "[15.589867  15.559178  15.529664  15.500809  15.473073  15.446452\n",
            " 15.420934  15.396519  15.373197  15.350961  15.329804  15.309722\n",
            " 15.290707  15.272753  15.2558565 15.240008  15.225206  15.211443\n",
            " 15.198715  15.187017  15.176345  15.166695  15.158065  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215108  15.229828  15.245749  15.262888  15.281267\n",
            " 15.300904  15.321823  15.344039  15.367578  15.392457  15.4187\n",
            " 15.446327  15.475359  15.505817  15.525191  15.555084  15.58607\n",
            " 15.618151 ]\n",
            "step 74\n",
            "[15.589868  15.559179  15.529664  15.50081   15.473072  15.446452\n",
            " 15.420934  15.396519  15.373195  15.350961  15.329804  15.309721\n",
            " 15.290708  15.272753  15.2558565 15.240008  15.225206  15.211444\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158065  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189196\n",
            " 15.2015705 15.215109  15.229828  15.2457485 15.262889  15.281267\n",
            " 15.300904  15.321822  15.344039  15.367578  15.392458  15.418701\n",
            " 15.446328  15.47536   15.505817  15.525191  15.555083  15.586071\n",
            " 15.618151 ]\n",
            "step 75\n",
            "[15.589868  15.559179  15.529665  15.500811  15.473073  15.446451\n",
            " 15.420935  15.396519  15.373196  15.350961  15.329803  15.309723\n",
            " 15.290708  15.272753  15.2558565 15.240008  15.225206  15.211444\n",
            " 15.198716  15.1870165 15.176345  15.166696  15.158065  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177967  15.189195\n",
            " 15.20157   15.215108  15.229828  15.2457485 15.262889  15.281266\n",
            " 15.300905  15.321821  15.34404   15.367578  15.392458  15.4187\n",
            " 15.446326  15.475359  15.505816  15.525191  15.555083  15.586071\n",
            " 15.618152 ]\n",
            "step 76\n",
            "[15.589867  15.559178  15.529664  15.50081   15.473073  15.446451\n",
            " 15.420935  15.39652   15.373196  15.350961  15.3298025 15.309721\n",
            " 15.290707  15.272753  15.2558565 15.240008  15.225206  15.211443\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158065  15.150452\n",
            " 15.143852  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177967  15.189195\n",
            " 15.2015705 15.215108  15.229828  15.245749  15.262889  15.281267\n",
            " 15.300905  15.321822  15.344039  15.3675785 15.392458  15.418701\n",
            " 15.446326  15.475359  15.505816  15.525191  15.555085  15.586069\n",
            " 15.618151 ]\n",
            "step 77\n",
            "[15.589867  15.559176  15.529665  15.500811  15.473074  15.446451\n",
            " 15.420935  15.396519  15.373196  15.350961  15.329803  15.309722\n",
            " 15.290708  15.272753  15.2558565 15.240008  15.225206  15.211444\n",
            " 15.198714  15.1870165 15.176345  15.166696  15.158065  15.150451\n",
            " 15.143852  15.1382675 15.133695  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215108  15.229829  15.2457485 15.262888  15.281267\n",
            " 15.300905  15.321823  15.34404   15.367578  15.392457  15.4187\n",
            " 15.446327  15.475359  15.505816  15.525193  15.555084  15.586071\n",
            " 15.618152 ]\n",
            "step 78\n",
            "[15.589866  15.559177  15.529663  15.500809  15.473073  15.446451\n",
            " 15.420935  15.39652   15.373196  15.350961  15.3298025 15.30972\n",
            " 15.290708  15.272753  15.2558565 15.240009  15.225206  15.211444\n",
            " 15.198715  15.1870165 15.176345  15.166696  15.158065  15.150451\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177967  15.189195\n",
            " 15.20157   15.215108  15.229828  15.245749  15.262889  15.281267\n",
            " 15.300905  15.321822  15.344039  15.3675785 15.392457  15.418701\n",
            " 15.446327  15.475359  15.5058155 15.525192  15.555085  15.586071\n",
            " 15.618153 ]\n",
            "step 79\n",
            "[15.589866  15.559177  15.529664  15.500809  15.473074  15.446451\n",
            " 15.420935  15.396519  15.373196  15.350959  15.329803  15.309722\n",
            " 15.290706  15.272754  15.2558565 15.240009  15.225206  15.211444\n",
            " 15.198715  15.1870165 15.176345  15.166696  15.158065  15.150451\n",
            " 15.143853  15.1382675 15.133695  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215108  15.229829  15.245749  15.262889  15.281267\n",
            " 15.300905  15.321823  15.34404   15.367578  15.392457  15.4187\n",
            " 15.446327  15.475359  15.505816  15.52519   15.555083  15.586071\n",
            " 15.618151 ]\n",
            "step 80\n",
            "[15.589866  15.559177  15.529663  15.50081   15.473073  15.446451\n",
            " 15.420936  15.396519  15.373196  15.35096   15.329803  15.309721\n",
            " 15.290707  15.272754  15.2558565 15.240009  15.225206  15.211444\n",
            " 15.198715  15.1870165 15.176345  15.166696  15.158066  15.150451\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189195\n",
            " 15.20157   15.215109  15.229828  15.2457485 15.262888  15.281267\n",
            " 15.300905  15.321822  15.344039  15.3675785 15.392457  15.418701\n",
            " 15.446326  15.47536   15.505816  15.525192  15.555082  15.586072\n",
            " 15.618151 ]\n",
            "step 81\n",
            "[15.589867  15.559177  15.529665  15.50081   15.473073  15.446451\n",
            " 15.420935  15.39652   15.373196  15.35096   15.329803  15.309721\n",
            " 15.290707  15.272754  15.2558565 15.240009  15.225206  15.211444\n",
            " 15.198715  15.1870165 15.176345  15.166696  15.158065  15.150451\n",
            " 15.143853  15.1382675 15.133695  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215109  15.229829  15.245749  15.262887  15.281268\n",
            " 15.300905  15.321822  15.344039  15.3675785 15.392457  15.418701\n",
            " 15.446327  15.47536   15.505816  15.525192  15.555082  15.586071\n",
            " 15.618151 ]\n",
            "step 82\n",
            "[15.589866  15.559177  15.529664  15.500808  15.473072  15.446452\n",
            " 15.420935  15.396521  15.373196  15.35096   15.3298025 15.309721\n",
            " 15.290707  15.272754  15.2558565 15.240008  15.225206  15.211443\n",
            " 15.198714  15.1870165 15.176345  15.166695  15.158065  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215108  15.229828  15.2457485 15.262888  15.281267\n",
            " 15.300904  15.321823  15.344039  15.3675785 15.392457  15.418701\n",
            " 15.446327  15.475359  15.505817  15.525192  15.555082  15.586071\n",
            " 15.618152 ]\n",
            "step 83\n",
            "[15.589866  15.559176  15.529665  15.500809  15.473073  15.446451\n",
            " 15.420935  15.39652   15.373196  15.35096   15.329803  15.309721\n",
            " 15.290707  15.272754  15.2558565 15.240009  15.225207  15.211443\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158065  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.15888   15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215109  15.229829  15.245749  15.262888  15.281267\n",
            " 15.300904  15.321822  15.344039  15.367578  15.392457  15.418701\n",
            " 15.446327  15.475359  15.505817  15.525192  15.555083  15.586071\n",
            " 15.618152 ]\n",
            "step 84\n",
            "[15.589866  15.559177  15.529664  15.500809  15.473072  15.446451\n",
            " 15.420935  15.396519  15.373196  15.35096   15.329803  15.309722\n",
            " 15.290707  15.272754  15.2558565 15.240009  15.225206  15.211443\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158065  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167866  15.177966  15.189195\n",
            " 15.2015705 15.215108  15.229829  15.245749  15.262887  15.281267\n",
            " 15.300904  15.321823  15.344039  15.3675785 15.392457  15.4187\n",
            " 15.446326  15.475359  15.505816  15.525193  15.555085  15.586069\n",
            " 15.618152 ]\n",
            "step 85\n",
            "[15.589867  15.559177  15.529664  15.500809  15.473073  15.446451\n",
            " 15.420934  15.39652   15.373196  15.35096   15.329804  15.309722\n",
            " 15.290707  15.272753  15.2558565 15.240009  15.225206  15.211443\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158065  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215109  15.229829  15.245749  15.262888  15.281267\n",
            " 15.300904  15.321823  15.344038  15.367578  15.392457  15.4187\n",
            " 15.446327  15.475359  15.505816  15.525193  15.555084  15.58607\n",
            " 15.618151 ]\n",
            "step 86\n",
            "[15.589867  15.559178  15.529665  15.500809  15.473073  15.446452\n",
            " 15.420934  15.396519  15.373197  15.35096   15.329803  15.309721\n",
            " 15.290707  15.272754  15.2558565 15.240009  15.225206  15.211443\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158065  15.150451\n",
            " 15.143853  15.1382675 15.133695  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.15888   15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215108  15.229829  15.245749  15.262888  15.281267\n",
            " 15.300904  15.321823  15.344039  15.3675785 15.392457  15.418701\n",
            " 15.446327  15.475359  15.505816  15.525193  15.555083  15.586069\n",
            " 15.618152 ]\n",
            "step 87\n",
            "[15.589867  15.559177  15.529664  15.50081   15.473072  15.446452\n",
            " 15.420935  15.39652   15.373196  15.35096   15.329803  15.309722\n",
            " 15.290707  15.272754  15.2558565 15.240009  15.225206  15.211443\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158066  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215109  15.229828  15.245749  15.262887  15.281267\n",
            " 15.300904  15.321822  15.344039  15.3675785 15.392458  15.4187\n",
            " 15.446326  15.47536   15.505817  15.525192  15.555083  15.586071\n",
            " 15.618152 ]\n",
            "step 88\n",
            "[15.589866  15.559178  15.529665  15.500809  15.473073  15.446452\n",
            " 15.420934  15.396519  15.373196  15.35096   15.329804  15.30972\n",
            " 15.290707  15.272754  15.255856  15.240009  15.225207  15.211443\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158065  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.15888   15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215109  15.229829  15.2457485 15.262887  15.281267\n",
            " 15.300904  15.321822  15.344039  15.3675785 15.392458  15.4187\n",
            " 15.446327  15.47536   15.505817  15.52519   15.555083  15.586069\n",
            " 15.618152 ]\n",
            "step 89\n",
            "[15.589866  15.559178  15.529664  15.50081   15.473072  15.446451\n",
            " 15.420934  15.396519  15.373196  15.350961  15.329804  15.309722\n",
            " 15.290707  15.272753  15.255856  15.240009  15.225206  15.211443\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158065  15.150451\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215108  15.229829  15.245749  15.262888  15.281268\n",
            " 15.300904  15.321823  15.34404   15.3675785 15.392458  15.4187\n",
            " 15.446328  15.475359  15.505816  15.52519   15.555083  15.58607\n",
            " 15.618151 ]\n",
            "step 90\n",
            "[15.589866  15.559179  15.529664  15.50081   15.473072  15.446451\n",
            " 15.420934  15.396519  15.373196  15.35096   15.329804  15.309721\n",
            " 15.290707  15.272754  15.2558565 15.240008  15.225206  15.211443\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158065  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215109  15.229828  15.245749  15.262888  15.281267\n",
            " 15.300904  15.321823  15.344039  15.367578  15.392457  15.418701\n",
            " 15.446328  15.475359  15.505816  15.52519   15.555084  15.586071\n",
            " 15.618152 ]\n",
            "step 91\n",
            "[15.589866  15.559179  15.529664  15.50081   15.473072  15.446452\n",
            " 15.420934  15.396519  15.373196  15.35096   15.329803  15.309722\n",
            " 15.290707  15.272754  15.255856  15.240008  15.225207  15.211443\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158065  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.15888   15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215109  15.229829  15.2457485 15.262888  15.281267\n",
            " 15.300904  15.321822  15.344039  15.3675785 15.392457  15.4187\n",
            " 15.446327  15.47536   15.505817  15.525191  15.555085  15.58607\n",
            " 15.618152 ]\n",
            "step 92\n",
            "[15.589867  15.559177  15.529664  15.50081   15.473072  15.446451\n",
            " 15.420934  15.396519  15.373195  15.35096   15.329803  15.309721\n",
            " 15.290707  15.272754  15.2558565 15.240008  15.225206  15.211443\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158065  15.150451\n",
            " 15.143853  15.1382675 15.133695  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215109  15.229828  15.245749  15.262888  15.281268\n",
            " 15.300904  15.321823  15.344039  15.3675785 15.392458  15.4187\n",
            " 15.446327  15.475359  15.505817  15.525192  15.555086  15.58607\n",
            " 15.618151 ]\n",
            "step 93\n",
            "[15.589867  15.559177  15.529664  15.50081   15.473072  15.446451\n",
            " 15.420935  15.396519  15.373196  15.35096   15.329804  15.30972\n",
            " 15.290707  15.272754  15.2558565 15.240008  15.225207  15.211443\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158066  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130136  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215108  15.229829  15.2457485 15.262888  15.281267\n",
            " 15.300904  15.321822  15.344039  15.367578  15.392458  15.418701\n",
            " 15.446327  15.475359  15.505816  15.525191  15.555084  15.586071\n",
            " 15.618151 ]\n",
            "step 94\n",
            "[15.589868  15.559178  15.529665  15.50081   15.473073  15.44645\n",
            " 15.420935  15.39652   15.373196  15.350961  15.329803  15.309722\n",
            " 15.290707  15.272754  15.255856  15.240008  15.225206  15.211443\n",
            " 15.198715  15.1870165 15.176345  15.166696  15.158065  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189195\n",
            " 15.2015705 15.21511   15.229829  15.245749  15.262888  15.281268\n",
            " 15.300904  15.321823  15.344039  15.367578  15.392457  15.4187\n",
            " 15.446327  15.47536   15.505817  15.525191  15.555086  15.586071\n",
            " 15.618153 ]\n",
            "step 95\n",
            "[15.589867  15.559179  15.529665  15.500809  15.473073  15.446451\n",
            " 15.420934  15.396519  15.373195  15.35096   15.329804  15.309722\n",
            " 15.290707  15.272754  15.2558565 15.240008  15.225207  15.211443\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158065  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.15888   15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215109  15.229829  15.245749  15.262888  15.281267\n",
            " 15.300904  15.321822  15.344039  15.367578  15.392458  15.4187\n",
            " 15.446327  15.475359  15.505816  15.525191  15.555084  15.586071\n",
            " 15.618151 ]\n",
            "step 96\n",
            "[15.589867  15.559178  15.529664  15.50081   15.473072  15.44645\n",
            " 15.420935  15.39652   15.373196  15.350961  15.329803  15.309721\n",
            " 15.290707  15.272754  15.2558565 15.240008  15.225207  15.211443\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158065  15.150451\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.15888   15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215109  15.229829  15.2457485 15.262887  15.281267\n",
            " 15.300904  15.321823  15.344039  15.367578  15.392457  15.4187\n",
            " 15.446327  15.47536   15.505817  15.525191  15.555086  15.58607\n",
            " 15.618152 ]\n",
            "step 97\n",
            "[15.589867  15.559179  15.529665  15.500811  15.473073  15.44645\n",
            " 15.420934  15.396519  15.373195  15.35096   15.329804  15.309721\n",
            " 15.290707  15.272754  15.255856  15.240008  15.225206  15.211443\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158066  15.150451\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215109  15.229829  15.2457485 15.262888  15.281268\n",
            " 15.300905  15.321823  15.344039  15.367577  15.392457  15.418701\n",
            " 15.446327  15.475359  15.505816  15.525191  15.555085  15.58607\n",
            " 15.618152 ]\n",
            "step 98\n",
            "[15.589868  15.559178  15.529664  15.50081   15.473072  15.44645\n",
            " 15.420935  15.39652   15.373196  15.350961  15.329803  15.309722\n",
            " 15.290707  15.272754  15.2558565 15.240008  15.225206  15.211443\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158065  15.150451\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215109  15.229828  15.2457485 15.262888  15.281267\n",
            " 15.300904  15.321822  15.344039  15.367578  15.392458  15.4187\n",
            " 15.446327  15.475359  15.505817  15.525191  15.555083  15.586071\n",
            " 15.618153 ]\n",
            "step 99\n",
            "[15.589867  15.559177  15.529665  15.500811  15.473073  15.446451\n",
            " 15.420934  15.396519  15.373195  15.350961  15.329804  15.309722\n",
            " 15.290707  15.272754  15.2558565 15.240008  15.225207  15.211443\n",
            " 15.198715  15.1870165 15.176345  15.166695  15.158065  15.150452\n",
            " 15.143853  15.1382675 15.133695  15.130135  15.127589  15.12606\n",
            " 15.125549  15.126061  15.127602  15.130177  15.133794  15.138462\n",
            " 15.144192  15.150993  15.158881  15.167867  15.177966  15.189195\n",
            " 15.2015705 15.215108  15.229829  15.2457485 15.262888  15.281267\n",
            " 15.300904  15.321823  15.344039  15.367578  15.392457  15.4187\n",
            " 15.446327  15.475359  15.505816  15.525192  15.555083  15.58607\n",
            " 15.618152 ]\n",
            "nll - p_nll -22.011673\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUy7yAg2mJyR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "outputId": "df270c1c-f2ed-4fe3-b80b-01386a7844b0"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from scipy.interpolate import InterpolatedUnivariateSpline\n",
        "fig, ax = plt.subplots(figsize=(8,6))\n",
        "\n",
        "shift_nll = p_nll-p_nll.min()\n",
        "inf_line = ax.plot(s_exp_scan, shift_nll ,\"b\",alpha=0.3)\n",
        "\n",
        "ax.set_ylim(bottom=0)\n",
        "ax.set_xlim(left=s_exp_scan.min(), right=s_exp_scan.max())\n",
        "\n",
        "ax.set_xlabel(\"$s$ parameter of interest\")\n",
        "ax.set_ylabel(r\"profiled likelihood $\\Delta(\\mathcal{-\\ln L})$\")\n",
        "\n",
        "ax.legend((inf_line), (\"inference-aware\"),\n",
        "          loc=\"upper center\",frameon=False)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f614e8ae160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB1Hne9PBhyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}