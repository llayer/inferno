{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "inferno_TF1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNas6ByJBwivRUeyGWkT0WR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/llayer/inferno/blob/master/inferno_TF1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dos0UMhvLxnd",
        "colab_type": "text"
      },
      "source": [
        "#INFERNO with TF 1.x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ntc5Urbt1knH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 939
        },
        "outputId": "9f85a663-b19f-4976-c5de-4149843d66ca"
      },
      "source": [
        "!pip install tensorflow==1.10.1"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/04/7e/a484776c73b1431f2b077e13801531e966113492552194fe721e6ef88d5d/tensorflow-1.10.1-cp36-cp36m-manylinux1_x86_64.whl (58.4MB)\n",
            "\u001b[K     |████████████████████████████████| 58.4MB 74kB/s \n",
            "\u001b[?25hCollecting setuptools<=39.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/10/79282747f9169f21c053c562a0baa21815a8c7879be97abd930dbcf862e8/setuptools-39.1.0-py2.py3-none-any.whl (566kB)\n",
            "\u001b[K     |████████████████████████████████| 573kB 43.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1) (0.3.3)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1) (1.27.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1) (1.1.0)\n",
            "Collecting tensorboard<1.11.0,>=1.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c6/17/ecd918a004f297955c30b4fffbea100b1606c225dbf0443264012773c3ff/tensorboard-1.10.0-py3-none-any.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 43.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1) (0.34.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1) (0.8.1)\n",
            "Collecting numpy<=1.14.5,>=1.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/1e/116ad560de97694e2d0c1843a7a0075cc9f49e922454d32f49a80eb6f1f2/numpy-1.14.5-cp36-cp36m-manylinux1_x86_64.whl (12.2MB)\n",
            "\u001b[K     |████████████████████████████████| 12.2MB 50.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1) (3.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1) (1.12.0)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.10.1) (0.9.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.1) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow==1.10.1) (1.0.0)\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: xarray 0.15.1 has requirement setuptools>=41.2, but you'll have setuptools 39.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: spacy 2.2.4 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: plotnine 0.6.0 has requirement numpy>=1.16.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: imgaug 0.2.9 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-auth 1.7.2 has requirement setuptools>=40.3.0, but you'll have setuptools 39.1.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 1.0.60 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: cvxpy 1.0.28 has requirement numpy>=1.15, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: blis 0.4.1 has requirement numpy>=1.15.0, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.0.1 has requirement numpy>=1.16, but you'll have numpy 1.14.5 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: setuptools, numpy, tensorboard, tensorflow\n",
            "  Found existing installation: setuptools 46.0.0\n",
            "    Uninstalling setuptools-46.0.0:\n",
            "      Successfully uninstalled setuptools-46.0.0\n",
            "  Found existing installation: numpy 1.18.2\n",
            "    Uninstalling numpy-1.18.2:\n",
            "      Successfully uninstalled numpy-1.18.2\n",
            "  Found existing installation: tensorboard 2.2.0\n",
            "    Uninstalling tensorboard-2.2.0:\n",
            "      Successfully uninstalled tensorboard-2.2.0\n",
            "  Found existing installation: tensorflow 2.2.0rc2\n",
            "    Uninstalling tensorflow-2.2.0rc2:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc2\n",
            "Successfully installed numpy-1.14.5 setuptools-39.1.0 tensorboard-1.10.0 tensorflow-1.10.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "pkg_resources"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dpI_G7X2PQN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "fca4e4d9-1052-4c24-c17b-f4abf6269355"
      },
      "source": [
        "!pip install tensorflow-probability==0.3.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-probability==0.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/61/401c24a0062f406d5b8d9014210970f977a571225543d151fdebbeab42a1/tensorflow_probability-0.3.0-py2.py3-none-any.whl (509kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<=1.14.5,>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.3.0) (1.14.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.3.0) (1.12.0)\n",
            "Requirement already satisfied: tensorflow>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-probability==0.3.0) (1.10.1)\n",
            "Requirement already satisfied: tensorboard<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow-probability==0.3.0) (1.10.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow-probability==0.3.0) (1.27.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow-probability==0.3.0) (0.3.3)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow-probability==0.3.0) (3.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow-probability==0.3.0) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow-probability==0.3.0) (0.34.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow-probability==0.3.0) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow-probability==0.3.0) (0.9.0)\n",
            "Requirement already satisfied: setuptools<=39.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=1.10.0->tensorflow-probability==0.3.0) (39.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow>=1.10.0->tensorflow-probability==0.3.0) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.11.0,>=1.10.0->tensorflow>=1.10.0->tensorflow-probability==0.3.0) (3.2.1)\n",
            "Installing collected packages: tensorflow-probability\n",
            "  Found existing installation: tensorflow-probability 0.9.0\n",
            "    Uninstalling tensorflow-probability-0.9.0:\n",
            "      Successfully uninstalled tensorflow-probability-0.9.0\n",
            "Successfully installed tensorflow-probability-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQE8UZXr7IXN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import trange"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcnCg-bW1rOw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "8b2dbbac-4867-4ee4-acc3-5603f34a2ba4"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUMwFsKx2BF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ge = tf.contrib.graph_editor\n",
        "ds = tfp.distributions"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9l614mn2JBY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "9e2bc393-54ce-4bc0-ff2f-a8ac932c25e7"
      },
      "source": [
        "!pip install git+https://github.com/pablodecm/neyman.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/pablodecm/neyman.git\n",
            "  Cloning https://github.com/pablodecm/neyman.git to /tmp/pip-req-build-tfu6xjlk\n",
            "  Running command git clone -q https://github.com/pablodecm/neyman.git /tmp/pip-req-build-tfu6xjlk\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from neyman==0.0.1) (1.14.5)\n",
            "Collecting edward>=1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/59/5831fa77a01a7b16d53626966b06e89064b1696166d74e856ef3eefb3119/edward-1.3.5.tar.gz (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 2.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from edward>=1.3->neyman==0.0.1) (1.12.0)\n",
            "Building wheels for collected packages: neyman, edward\n",
            "  Building wheel for neyman (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for neyman: filename=neyman-0.0.1-cp36-none-any.whl size=5293 sha256=7ccfb28ff2268b8c4f4756f61dea20ef750c5e10129b98717d0542a2a9a158ed\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-z244rwd7/wheels/c5/02/c3/daef8b8e644e4dbee14ad489db8f51c08838dfcc1a621ed3ea\n",
            "  Building wheel for edward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for edward: filename=edward-1.3.5-cp36-none-any.whl size=90388 sha256=c3120050457dcc1540f7bea299502807a751d27bb1f05061322f04c2936dbc1c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/61/0c/1f36f3f0c629d1b7a24d042d2c37015a66c091729c95dd8425\n",
            "Successfully built neyman edward\n",
            "Installing collected packages: edward, neyman\n",
            "Successfully installed edward-1.3.5 neyman-0.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/google/colab/_pip.py:89: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.6/dist-packages/edward-1.3.5.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
            "  for line in open(toplevel):\n",
            "/usr/local/lib/python3.6/dist-packages/google/colab/_pip.py:89: ResourceWarning: unclosed file <_io.TextIOWrapper name='/usr/local/lib/python3.6/dist-packages/neyman-0.0.1.dist-info/top_level.txt' mode='r' encoding='UTF-8'>\n",
            "  for line in open(toplevel):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wx0qwbnG3U-T",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d4890a7c-8500-42f4-9788-083abde99557"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hz5W-NMz3WSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from neyman.inferences import batch_hessian"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnLpbo443dkF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5d3fa798-69f3-49b7-df5e-e1b656f2ae6f"
      },
      "source": [
        "! git clone https://github.com/llayer/paper-inferno"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'paper-inferno'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects:   6% (1/15)\u001b[K\rremote: Counting objects:  13% (2/15)\u001b[K\rremote: Counting objects:  20% (3/15)\u001b[K\rremote: Counting objects:  26% (4/15)\u001b[K\rremote: Counting objects:  33% (5/15)\u001b[K\rremote: Counting objects:  40% (6/15)\u001b[K\rremote: Counting objects:  46% (7/15)\u001b[K\rremote: Counting objects:  53% (8/15)\u001b[K\rremote: Counting objects:  60% (9/15)\u001b[K\rremote: Counting objects:  66% (10/15)\u001b[K\rremote: Counting objects:  73% (11/15)\u001b[K\rremote: Counting objects:  80% (12/15)\u001b[K\rremote: Counting objects:  86% (13/15)\u001b[K\rremote: Counting objects:  93% (14/15)\u001b[K\rremote: Counting objects: 100% (15/15)\u001b[K\rremote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects:   6% (1/15)\u001b[K\rremote: Compressing objects:  13% (2/15)\u001b[K\rremote: Compressing objects:  20% (3/15)\u001b[K\rremote: Compressing objects:  26% (4/15)\u001b[K\rremote: Compressing objects:  33% (5/15)\u001b[K\rremote: Compressing objects:  40% (6/15)\u001b[K\rremote: Compressing objects:  46% (7/15)\u001b[K\rremote: Compressing objects:  53% (8/15)\u001b[K\rremote: Compressing objects:  60% (9/15)\u001b[K\rremote: Compressing objects:  66% (10/15)\u001b[K\rremote: Compressing objects:  73% (11/15)\u001b[K\rremote: Compressing objects:  80% (12/15)\u001b[K\rremote: Compressing objects:  86% (13/15)\u001b[K\rremote: Compressing objects:  93% (14/15)\u001b[K\rremote: Compressing objects: 100% (15/15)\u001b[K\rremote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "Receiving objects:   0% (1/1063)   \rReceiving objects:   1% (11/1063)   \rReceiving objects:   2% (22/1063)   \rReceiving objects:   3% (32/1063)   \rReceiving objects:   4% (43/1063)   \rReceiving objects:   5% (54/1063)   \rReceiving objects:   6% (64/1063)   \rReceiving objects:   7% (75/1063)   \rReceiving objects:   8% (86/1063)   \rReceiving objects:   9% (96/1063)   \rReceiving objects:  10% (107/1063)   \rReceiving objects:  11% (117/1063)   \rReceiving objects:  12% (128/1063)   \rReceiving objects:  13% (139/1063)   \rReceiving objects:  14% (149/1063)   \rReceiving objects:  15% (160/1063)   \rReceiving objects:  16% (171/1063)   \rReceiving objects:  17% (181/1063)   \rReceiving objects:  18% (192/1063)   \rReceiving objects:  19% (202/1063)   \rReceiving objects:  20% (213/1063)   \rReceiving objects:  21% (224/1063)   \rReceiving objects:  22% (234/1063)   \rReceiving objects:  23% (245/1063)   \rReceiving objects:  24% (256/1063)   \rReceiving objects:  25% (266/1063)   \rReceiving objects:  26% (277/1063)   \rReceiving objects:  27% (288/1063)   \rReceiving objects:  28% (298/1063)   \rReceiving objects:  29% (309/1063)   \rReceiving objects:  30% (319/1063)   \rReceiving objects:  31% (330/1063)   \rReceiving objects:  32% (341/1063)   \rReceiving objects:  33% (351/1063)   \rReceiving objects:  34% (362/1063)   \rReceiving objects:  35% (373/1063)   \rReceiving objects:  36% (383/1063)   \rReceiving objects:  37% (394/1063)   \rReceiving objects:  38% (404/1063)   \rReceiving objects:  39% (415/1063)   \rReceiving objects:  40% (426/1063)   \rReceiving objects:  41% (436/1063)   \rReceiving objects:  42% (447/1063)   \rReceiving objects:  43% (458/1063)   \rReceiving objects:  44% (468/1063)   \rReceiving objects:  45% (479/1063)   \rReceiving objects:  46% (489/1063)   \rReceiving objects:  47% (500/1063)   \rReceiving objects:  48% (511/1063)   \rReceiving objects:  49% (521/1063)   \rReceiving objects:  50% (532/1063)   \rReceiving objects:  51% (543/1063)   \rReceiving objects:  52% (553/1063)   \rReceiving objects:  53% (564/1063)   \rReceiving objects:  54% (575/1063)   \rReceiving objects:  55% (585/1063)   \rReceiving objects:  56% (596/1063)   \rReceiving objects:  57% (606/1063)   \rReceiving objects:  58% (617/1063)   \rReceiving objects:  59% (628/1063)   \rReceiving objects:  60% (638/1063)   \rReceiving objects:  61% (649/1063)   \rReceiving objects:  62% (660/1063)   \rReceiving objects:  63% (670/1063)   \rReceiving objects:  64% (681/1063)   \rReceiving objects:  65% (691/1063)   \rReceiving objects:  66% (702/1063)   \rReceiving objects:  67% (713/1063)   \rReceiving objects:  68% (723/1063)   \rReceiving objects:  69% (734/1063)   \rReceiving objects:  70% (745/1063)   \rReceiving objects:  71% (755/1063)   \rReceiving objects:  72% (766/1063)   \rReceiving objects:  73% (776/1063)   \rReceiving objects:  74% (787/1063)   \rReceiving objects:  75% (798/1063)   \rremote: Total 1063 (delta 8), reused 7 (delta 0), pack-reused 1048\u001b[K\n",
            "Receiving objects:  76% (808/1063)   \rReceiving objects:  77% (819/1063)   \rReceiving objects:  78% (830/1063)   \rReceiving objects:  79% (840/1063)   \rReceiving objects:  80% (851/1063)   \rReceiving objects:  81% (862/1063)   \rReceiving objects:  82% (872/1063)   \rReceiving objects:  83% (883/1063)   \rReceiving objects:  84% (893/1063)   \rReceiving objects:  85% (904/1063)   \rReceiving objects:  86% (915/1063)   \rReceiving objects:  87% (925/1063)   \rReceiving objects:  88% (936/1063)   \rReceiving objects:  89% (947/1063)   \rReceiving objects:  90% (957/1063)   \rReceiving objects:  91% (968/1063)   \rReceiving objects:  92% (978/1063)   \rReceiving objects:  93% (989/1063)   \rReceiving objects:  94% (1000/1063)   \rReceiving objects:  95% (1010/1063)   \rReceiving objects:  96% (1021/1063)   \rReceiving objects:  97% (1032/1063)   \rReceiving objects:  98% (1042/1063)   \rReceiving objects:  99% (1053/1063)   \rReceiving objects: 100% (1063/1063)   \rReceiving objects: 100% (1063/1063), 1.56 MiB | 20.44 MiB/s, done.\n",
            "Resolving deltas:   0% (0/687)   \rResolving deltas:   4% (29/687)   \rResolving deltas:  19% (132/687)   \rResolving deltas:  20% (143/687)   \rResolving deltas:  21% (148/687)   \rResolving deltas:  25% (172/687)   \rResolving deltas:  26% (180/687)   \rResolving deltas:  27% (189/687)   \rResolving deltas:  29% (204/687)   \rResolving deltas:  31% (213/687)   \rResolving deltas:  32% (220/687)   \rResolving deltas:  33% (230/687)   \rResolving deltas:  37% (255/687)   \rResolving deltas:  38% (264/687)   \rResolving deltas:  39% (268/687)   \rResolving deltas:  40% (280/687)   \rResolving deltas:  41% (284/687)   \rResolving deltas:  42% (289/687)   \rResolving deltas:  43% (301/687)   \rResolving deltas:  44% (303/687)   \rResolving deltas:  47% (325/687)   \rResolving deltas:  48% (336/687)   \rResolving deltas:  49% (337/687)   \rResolving deltas:  55% (379/687)   \rResolving deltas:  59% (412/687)   \rResolving deltas:  60% (413/687)   \rResolving deltas:  61% (422/687)   \rResolving deltas:  63% (433/687)   \rResolving deltas:  64% (444/687)   \rResolving deltas:  66% (457/687)   \rResolving deltas:  67% (464/687)   \rResolving deltas:  68% (469/687)   \rResolving deltas:  70% (481/687)   \rResolving deltas:  71% (488/687)   \rResolving deltas:  73% (503/687)   \rResolving deltas:  74% (512/687)   \rResolving deltas:  77% (533/687)   \rResolving deltas:  84% (578/687)   \rResolving deltas:  89% (613/687)   \rResolving deltas:  90% (619/687)   \rResolving deltas:  91% (626/687)   \rResolving deltas:  92% (635/687)   \rResolving deltas: 100% (687/687)   \rResolving deltas: 100% (687/687), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KC2Jd8aC5ML0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f961895-03a2-40e5-ad83-457128394d5f"
      },
      "source": [
        "%cd paper-inferno/"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/paper-inferno\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wqA7hmq5uHn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "efc83637-d7d1-4844-f5da-53dfe758418f"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "code  environment.yml  LICENSE\tnotebooks  paper  README.md  setup.cfg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WSjOsOJd5vLu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f93d099-5d21-47b6-ae2f-3d8bd6085f23"
      },
      "source": [
        "%cd code"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/paper-inferno/code\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdBX_DHc5ykP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "2b533e67-4045-4443-894c-0cfea10dd5b9"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "benchmarking.py\t\t       synthetic_3D_inferno.py\n",
            "extended_model.py\t       synthetic_3D_train_cross_entropy.py\n",
            "fisher_matrix.py\t       synthetic_3D_train_inferno.py\n",
            "summary_statistic_computer.py  template_likelihood.py\n",
            "synthetic_3D_cross_entropy.py  template_model.py\n",
            "synthetic_3D_example.py        train_helpers.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpqtILTY5zlN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from neyman.inferences import batch_hessian\n",
        "\n",
        "from synthetic_3D_example import SyntheticThreeDimExample\n",
        "from train_helpers import MixtureBatcher\n",
        "import os\n",
        "import json\n",
        "import itertools as it\n",
        "from fisher_matrix import FisherMatrix\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl82hwRL53D1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k = tf.keras\n",
        "ds = tfp.distributions\n",
        "ge = tf.contrib.graph_editor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cpl7RlN46Jna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SyntheticThreeDimInferno(object):\n",
        "\n",
        "  def __init__(self, model_path, poi, pars, seed, aux={}):\n",
        "\n",
        "    tf.set_random_seed(seed)\n",
        "\n",
        "    self.problem = SyntheticThreeDimExample()\n",
        "\n",
        "    self.batcher = MixtureBatcher([\"sig\", \"bkg\"])\n",
        "\n",
        "    s_batch = self.batcher.batch[\"sig\"]\n",
        "    b_batch = self.problem.transform_bkg(self.batcher.batch[\"bkg\"])\n",
        "    b_sizes = [tf.shape(s_batch)[0], tf.shape(b_batch)[0]]\n",
        "    train_batch = tf.concat([s_batch, b_batch], axis=0,\n",
        "                            name=\"input_batch\")\n",
        "\n",
        "    k_init = \"he_normal\"\n",
        "    Dense = k.layers.Dense\n",
        "    self.nn_model = k.Sequential([Dense(units=100, activation=\"relu\",\n",
        "                                        kernel_initializer=k_init,\n",
        "                                        input_shape=(3,)),\n",
        "                                  Dense(units=100, activation=\"relu\",\n",
        "                                        kernel_initializer=k_init),\n",
        "                                  Dense(units=10, activation=\"linear\")])\n",
        "\n",
        "    self.logits = self.nn_model(train_batch)\n",
        "    self.temperature = tf.placeholder_with_default(1., shape=())\n",
        "    self.probs = tf.nn.softmax(self.logits / self.temperature)\n",
        "\n",
        "    s_probs, b_probs = tf.split(self.probs, b_sizes, axis=0)\n",
        "\n",
        "    s_counts = tf.reduce_mean(s_probs, axis=0)\n",
        "    b_counts = tf.reduce_mean(b_probs, axis=0),\n",
        "\n",
        "    self.exp_counts = tf.cast(self.problem.s_exp * s_counts +\n",
        "                              self.problem.b_exp * b_counts,\n",
        "                              dtype=tf.float64)\n",
        "\n",
        "    self.pois = ds.Poisson(self.exp_counts, name=\"poisson\")\n",
        "    self.asimov = tf.stop_gradient(self.exp_counts, name=\"asimov\")\n",
        "\n",
        "    self.nll = - tf.cast(tf.reduce_sum(self.pois.log_prob(self.asimov)),\n",
        "                         name=\"nll\", dtype=tf.float32)\n",
        "\n",
        "    all_pars = list(self.problem.all_pars.values())\n",
        "    self.hess_nll, self.grad_nll = batch_hessian(self.nll,\n",
        "                                                 all_pars)\n",
        "\n",
        "    self.aux = aux\n",
        "\n",
        "    self.nll_aux = {}\n",
        "    self.hess_nll_aux = {}\n",
        "    for par, dist in self.aux.items():\n",
        "        self.nll_aux[par] = -dist.log_prob(self.problem.all_pars[par])\n",
        "        self.hess_nll_aux[par], _ = batch_hessian(self.nll_aux[par],\n",
        "                                                  all_pars)\n",
        "\n",
        "    self.ext_nll = sum([self.hess_nll] + list(self.hess_nll_aux.values()))\n",
        "\n",
        "    self.cov_nll = self.cov_matrix(pars)\n",
        "    idx_poi = pars.index(poi)\n",
        "    self.loss = self.cov_nll[idx_poi, idx_poi]\n",
        "\n",
        "    # remove stop gradient after loss is computed\n",
        "    ge.edit.bypass(self.asimov.op)\n",
        "\n",
        "    self.lr = tf.placeholder(shape=(), dtype=tf.float32)\n",
        "    self.optimizer = tf.train.GradientDescentOptimizer(self.lr)\n",
        "    self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "    self.train_op = self.optimizer.minimize(\n",
        "        self.loss, global_step=self.global_step)\n",
        "\n",
        "    self.init_op = tf.global_variables_initializer()\n",
        "\n",
        "    self.model_path = model_path\n",
        "\n",
        "    if not os.path.exists(self.model_path):\n",
        "      os.makedirs(self.model_path)\n",
        "\n",
        "    json_str = self.nn_model.to_json()\n",
        "    with open(f'{self.model_path}/model.json', 'w') as f:\n",
        "      json.dump(json_str, f)\n",
        "\n",
        "    self.saver = tf.train.Saver()\n",
        "    self.history = {}\n",
        "\n",
        "  def cov_matrix(self, pars):\n",
        "\n",
        "    pars = tuple(pars)\n",
        "\n",
        "    indices = [list(self.problem.all_pars.keys()).index(par) for par in pars]\n",
        "    idx_subset = np.reshape(list(it.product(indices, indices)),\n",
        "                            (len(pars), len(pars), -1))\n",
        "    hess_subset = tf.gather_nd(self.ext_nll, idx_subset)\n",
        "    cov_nll = tf.matrix_inverse(hess_subset)\n",
        "\n",
        "    return cov_nll\n",
        "\n",
        "  def fit(self, n_epochs, lr, temperature, batch_size, seed, par_phs={}):\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "      train_arrays = sess.run(self.problem.train_data())\n",
        "      valid_arrays = sess.run(self.problem.valid_data())\n",
        "\n",
        "    phs_train = {self.lr: lr,\n",
        "                 self.temperature: temperature}\n",
        "\n",
        "    phs_val = {self.temperature: temperature}\n",
        "\n",
        "    rs = np.random.RandomState(seed=seed)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "\n",
        "      #writer = tf.summary.FileWriter('logs', sess.graph)\n",
        "\n",
        "      k.backend.set_session(sess)\n",
        "      sess.run(self.init_op)\n",
        "      batch_n = 0\n",
        "      with trange(n_epochs) as t:\n",
        "        for i in t:\n",
        "          shuffle_seed = rs.randint(np.iinfo(np.int32).max)\n",
        "          self.batcher.init_iterator(train_arrays,\n",
        "                                     batch_size=batch_size, seed=shuffle_seed)\n",
        "          while True:\n",
        "            try:\n",
        "              batch_n += 1\n",
        "\n",
        "              \n",
        "              loss_t, _ = sess.run([self.loss, self.train_op], phs_train)\n",
        "              print( loss_t )\n",
        "              print( self.loss.eval() )\n",
        "\n",
        "              \"\"\"\n",
        "              loss_t = sess.run(self.loss, phs_train)\n",
        "              print( \"Loss in first session run\" )\n",
        "              print(self.loss.eval())\n",
        "              print( \"Cov first session run\" )\n",
        "              print(self.cov_nll.eval())\n",
        "              sess.run(self.train_op, phs_train)\n",
        "              print( \"Loss in second session run\" )\n",
        "              print(self.loss.eval())\n",
        "              print( \"Cov second session run\" )\n",
        "              print(self.cov_nll.eval())\n",
        "              \"\"\"\n",
        "              self.history.setdefault(\"loss_train\", []).append(\n",
        "                  [batch_n, float(np.sqrt(loss_t))])\n",
        "            except tf.errors.OutOfRangeError:\n",
        "              break\n",
        "          # fix seed for validation set (no need to shuffle)\n",
        "          self.batcher.init_iterator(valid_arrays,\n",
        "                                     batch_size=batch_size, seed=20)\n",
        "          val_losses = []\n",
        "          while True:\n",
        "            try:\n",
        "              loss_t = sess.run([self.loss], phs_val)\n",
        "              val_losses.append(np.sqrt(loss_t))\n",
        "            except tf.errors.OutOfRangeError:\n",
        "              break\n",
        "          val_loss = np.mean(val_losses)\n",
        "          val_loss_std = np.std(val_losses, ddof=1)\n",
        "          t.set_postfix({\"mean_val_loss\": val_loss})\n",
        "          self.history.setdefault(\"loss_valid\", []).append(\n",
        "              [batch_n, float(val_loss)])\n",
        "          self.history.setdefault(\"loss_std_valid\", []).append(\n",
        "              [batch_n, float(val_loss_std)])\n",
        "\n",
        "      self.nn_model.save_weights(f'{self.model_path}/model.h5')\n",
        "      self.saver.save(sess, f'{self.model_path}/model.ckpt',\n",
        "                      global_step=self.global_step)\n",
        "      with open(f'{self.model_path}/history.json', 'w') as fp:\n",
        "        json.dump(self.history, fp)\n",
        "\n",
        "  def load_weights(self):\n",
        "    sess = tf.get_default_session()\n",
        "    last_ckpt = tf.train.latest_checkpoint(f'{self.model_path}')\n",
        "    print(\"loading_vars_from\", last_ckpt)\n",
        "    self.saver.restore(sess, last_ckpt)\n",
        "\n",
        "  def eval_hessian(self, temperature):\n",
        "\n",
        "    phs_val = {self.temperature: temperature}\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "      valid_arrays = sess.run(self.problem.valid_data())\n",
        "      self.load_weights()\n",
        "      self.batcher.init_iterator(valid_arrays,\n",
        "                                 batch_size=-1, seed=20)\n",
        "      hess, hess_aux = sess.run([self.hess_nll, self.hess_nll_aux], phs_val)\n",
        "      print(hess)\n",
        "\n",
        "\n",
        "    pars = list(self.problem.all_pars.keys())\n",
        "    fisher = FisherMatrix(hess, pars)\n",
        "    aux_fisher = FisherMatrix(sum(hess_aux.values()), pars)\n",
        "\n",
        "    return fisher, aux_fisher"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgO0Ajic6Pe8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cc3fb0fc-fa93-49ca-ddcd-51bac43b19b2"
      },
      "source": [
        "n_epochs = 5\n",
        "lr = 1e-6\n",
        "batch_size = 1000\n",
        "t_train = 0.1\n",
        "t_eval = 0.05\n",
        "\n",
        "n_inits = 100\n",
        "seed = 7\n",
        "\n",
        "pars = [\"s_exp\", \"r_dist\", \"b_rate\"]\n",
        "\n",
        "aux = {\"r_dist\": ds.Normal(loc=2.0, scale=0.4),\n",
        "       \"b_rate\": ds.Normal(loc=3.0, scale=1.)}\n",
        "\n",
        "\n",
        "inf_path = f\"inf_ne_{n_epochs}_lr_{lr}_bs_{batch_size}_t_{t_train}\"\n",
        "\n",
        "inferno = SyntheticThreeDimInferno(model_path=inf_path, poi=\"s_exp\",\n",
        "                                    pars=pars, seed=seed, aux=aux)\n",
        "\n",
        "inferno.fit(n_epochs=n_epochs, lr=lr, batch_size=batch_size,\n",
        "            temperature=t_train, seed=seed)\n",
        "\n",
        "inf_fisher, inf_aux_fisher = inferno.eval_hessian(temperature=t_eval)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/tf_inspect.py:75: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
            "  return _inspect.getargspec(target)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "840.94995\n",
            "882.85925\n",
            "884.2228\n",
            "854.3461\n",
            "851.02435\n",
            "864.0018\n",
            "816.5241\n",
            "811.79376\n",
            "715.1856\n",
            "837.3792\n",
            "687.2085\n",
            "854.9623\n",
            "823.84546\n",
            "861.7039\n",
            "779.91895\n",
            "846.92084\n",
            "645.08887\n",
            "817.533\n",
            "718.5205\n",
            "837.9859\n",
            "789.7924\n",
            "830.7519\n",
            "786.24713\n",
            "805.4109\n",
            "766.00305\n",
            "815.4682\n",
            "702.5723\n",
            "824.15314\n",
            "663.26434\n",
            "781.40894\n",
            "733.53796\n",
            "843.4815\n",
            "784.0845\n",
            "819.2976\n",
            "715.77673\n",
            "816.9187\n",
            "678.92523\n",
            "830.67267\n",
            "701.0779\n",
            "776.1544\n",
            "761.05145\n",
            "840.65717\n",
            "664.36597\n",
            "842.1871\n",
            "700.8301\n",
            "826.0392\n",
            "775.78735\n",
            "808.75543\n",
            "788.26306\n",
            "778.8504\n",
            "631.689\n",
            "797.07074\n",
            "701.0061\n",
            "819.16284\n",
            "732.91705\n",
            "811.4951\n",
            "748.68805\n",
            "843.276\n",
            "828.1642\n",
            "826.07434\n",
            "692.283\n",
            "826.7926\n",
            "793.3572\n",
            "821.2981\n",
            "647.0918\n",
            "836.2655\n",
            "576.26733\n",
            "833.21545\n",
            "720.9707\n",
            "820.2175\n",
            "725.1554\n",
            "851.19385\n",
            "627.8682\n",
            "812.8677\n",
            "803.03876\n",
            "794.16394\n",
            "630.6521\n",
            "800.01874\n",
            "730.9766\n",
            "800.96765\n",
            "591.69464\n",
            "801.944\n",
            "608.8696\n",
            "812.1859\n",
            "700.0051\n",
            "821.1863\n",
            "673.2455\n",
            "847.32697\n",
            "668.03564\n",
            "839.87146\n",
            "773.226\n",
            "817.23865\n",
            "613.7815\n",
            "822.8358\n",
            "567.7438\n",
            "807.341\n",
            "723.9433\n",
            "812.798\n",
            "776.7808\n",
            "828.9555\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 20%|██        | 1/5 [00:15<01:02, 15.53s/it, mean_val_loss=26.3]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "640.9411\n",
            "821.6849\n",
            "729.2724\n",
            "789.4684\n",
            "682.7132\n",
            "832.25916\n",
            "770.3797\n",
            "844.37024\n",
            "634.43304\n",
            "822.67065\n",
            "652.86664\n",
            "803.5826\n",
            "555.7286\n",
            "828.3774\n",
            "657.6948\n",
            "825.6535\n",
            "661.98065\n",
            "803.8457\n",
            "689.6128\n",
            "805.90216\n",
            "570.515\n",
            "793.208\n",
            "687.77325\n",
            "794.2541\n",
            "716.331\n",
            "798.86523\n",
            "764.9321\n",
            "781.7304\n",
            "582.2019\n",
            "773.16614\n",
            "633.6009\n",
            "836.07263\n",
            "660.28436\n",
            "791.0135\n",
            "672.397\n",
            "794.2207\n",
            "725.2072\n",
            "787.78705\n",
            "696.1729\n",
            "790.4462\n",
            "712.74286\n",
            "798.9607\n",
            "658.58887\n",
            "800.85724\n",
            "702.4296\n",
            "809.68024\n",
            "604.38385\n",
            "797.7996\n",
            "629.5391\n",
            "769.86346\n",
            "609.3294\n",
            "755.95245\n",
            "548.8711\n",
            "781.8096\n",
            "706.76624\n",
            "768.641\n",
            "720.7137\n",
            "769.3843\n",
            "542.3312\n",
            "771.105\n",
            "732.4841\n",
            "801.30646\n",
            "667.39386\n",
            "782.3083\n",
            "663.36896\n",
            "781.0351\n",
            "617.22485\n",
            "782.3598\n",
            "710.58185\n",
            "785.34143\n",
            "694.71387\n",
            "775.51666\n",
            "500.7253\n",
            "773.13\n",
            "656.4944\n",
            "782.9994\n",
            "653.4169\n",
            "782.0273\n",
            "599.7809\n",
            "764.7752\n",
            "629.4701\n",
            "785.3805\n",
            "819.6346\n",
            "771.167\n",
            "703.80554\n",
            "775.8679\n",
            "762.2848\n",
            "757.5077\n",
            "520.957\n",
            "750.6979\n",
            "619.80835\n",
            "771.08466\n",
            "619.08923\n",
            "769.9719\n",
            "646.8245\n",
            "786.86597\n",
            "602.45404\n",
            "741.99115\n",
            "608.83405\n",
            "781.28564\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 40%|████      | 2/5 [00:25<00:41, 13.90s/it, mean_val_loss=25.4]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "583.6518\n",
            "746.9593\n",
            "713.20465\n",
            "770.14703\n",
            "632.9477\n",
            "743.07166\n",
            "775.6168\n",
            "748.4634\n",
            "426.7404\n",
            "777.08234\n",
            "674.58215\n",
            "748.5558\n",
            "668.5632\n",
            "744.8795\n",
            "580.3534\n",
            "739.11163\n",
            "720.7561\n",
            "759.55444\n",
            "605.15155\n",
            "766.0337\n",
            "584.38354\n",
            "763.63153\n",
            "706.1939\n",
            "716.1916\n",
            "721.73047\n",
            "745.9146\n",
            "495.98846\n",
            "755.89014\n",
            "751.42975\n",
            "766.2052\n",
            "467.09277\n",
            "786.51697\n",
            "620.66547\n",
            "754.21716\n",
            "555.3419\n",
            "763.0724\n",
            "602.5069\n",
            "788.2986\n",
            "704.6636\n",
            "781.8334\n",
            "578.85846\n",
            "788.84045\n",
            "570.77325\n",
            "794.7168\n",
            "641.38715\n",
            "789.9568\n",
            "588.8081\n",
            "790.57056\n",
            "572.3375\n",
            "778.9756\n",
            "544.24945\n",
            "753.77466\n",
            "544.86316\n",
            "777.48645\n",
            "506.33383\n",
            "767.4134\n",
            "572.49817\n",
            "755.29456\n",
            "628.97345\n",
            "781.25916\n",
            "640.71747\n",
            "825.74335\n",
            "562.7727\n",
            "772.4592\n",
            "585.7395\n",
            "778.5832\n",
            "602.4987\n",
            "799.50116\n",
            "644.3079\n",
            "808.814\n",
            "640.44354\n",
            "786.83636\n",
            "594.4043\n",
            "792.66473\n",
            "638.35034\n",
            "798.20465\n",
            "702.21783\n",
            "766.9191\n",
            "497.46216\n",
            "805.67255\n",
            "565.43524\n",
            "782.02704\n",
            "554.136\n",
            "793.6892\n",
            "663.61005\n",
            "797.85254\n",
            "652.18915\n",
            "792.48224\n",
            "590.58484\n",
            "779.9272\n",
            "626.79\n",
            "781.95416\n",
            "565.0648\n",
            "789.5943\n",
            "660.8862\n",
            "754.8753\n",
            "517.7083\n",
            "769.1004\n",
            "713.2161\n",
            "783.11346\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 60%|██████    | 3/5 [00:35<00:25, 12.73s/it, mean_val_loss=24.7]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "518.39056\n",
            "771.947\n",
            "696.30286\n",
            "816.0959\n",
            "737.1734\n",
            "788.54755\n",
            "668.9889\n",
            "775.89545\n",
            "486.5506\n",
            "782.086\n",
            "644.5273\n",
            "785.6363\n",
            "625.21027\n",
            "765.9063\n",
            "645.3838\n",
            "769.59576\n",
            "593.0075\n",
            "747.276\n",
            "496.42355\n",
            "765.4325\n",
            "566.0729\n",
            "802.16223\n",
            "823.1319\n",
            "772.982\n",
            "546.0357\n",
            "747.8653\n",
            "611.1655\n",
            "763.0007\n",
            "620.3582\n",
            "772.3654\n",
            "671.1666\n",
            "740.14734\n",
            "592.4747\n",
            "766.9764\n",
            "700.42114\n",
            "785.41064\n",
            "516.37286\n",
            "760.657\n",
            "529.06635\n",
            "775.58704\n",
            "545.32367\n",
            "782.8907\n",
            "452.9998\n",
            "748.8415\n",
            "548.9161\n",
            "757.89185\n",
            "606.78564\n",
            "746.4153\n",
            "798.86426\n",
            "750.67145\n",
            "580.97516\n",
            "745.1661\n",
            "509.9293\n",
            "751.02167\n",
            "691.6973\n",
            "745.81226\n",
            "610.6265\n",
            "767.72144\n",
            "524.3637\n",
            "741.12915\n",
            "634.1179\n",
            "784.78125\n",
            "551.6136\n",
            "753.7084\n",
            "493.03604\n",
            "764.323\n",
            "458.77853\n",
            "767.80524\n",
            "630.1818\n",
            "778.92645\n",
            "504.3121\n",
            "777.20306\n",
            "602.35815\n",
            "762.2481\n",
            "448.23032\n",
            "771.28345\n",
            "551.7349\n",
            "756.18884\n",
            "562.27106\n",
            "739.7539\n",
            "644.72766\n",
            "763.5125\n",
            "669.2682\n",
            "777.8877\n",
            "641.249\n",
            "747.31934\n",
            "592.63544\n",
            "739.8306\n",
            "509.36652\n",
            "745.3142\n",
            "605.5749\n",
            "757.1082\n",
            "430.57938\n",
            "746.83044\n",
            "569.8827\n",
            "753.90845\n",
            "620.484\n",
            "784.2384\n",
            "707.1104\n",
            "767.3398\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " 80%|████████  | 4/5 [00:45<00:11, 11.92s/it, mean_val_loss=24.8]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "665.9164\n",
            "773.13025\n",
            "445.7698\n",
            "747.80023\n",
            "398.1087\n",
            "745.71826\n",
            "531.338\n",
            "749.4799\n",
            "599.2278\n",
            "798.8358\n",
            "490.86\n",
            "751.56177\n",
            "631.84485\n",
            "757.102\n",
            "601.6336\n",
            "795.5061\n",
            "634.6445\n",
            "790.40454\n",
            "503.36353\n",
            "752.13385\n",
            "489.97385\n",
            "768.47363\n",
            "457.4237\n",
            "748.05383\n",
            "509.76685\n",
            "759.63873\n",
            "506.3738\n",
            "761.54663\n",
            "566.6033\n",
            "762.2727\n",
            "445.2157\n",
            "764.42017\n",
            "517.8434\n",
            "768.8343\n",
            "592.44366\n",
            "768.8391\n",
            "641.1883\n",
            "751.9886\n",
            "436.42877\n",
            "766.93994\n",
            "549.7879\n",
            "786.8651\n",
            "624.478\n",
            "736.72784\n",
            "635.78235\n",
            "743.2137\n",
            "488.17532\n",
            "780.67413\n",
            "680.3915\n",
            "764.5177\n",
            "652.6474\n",
            "763.5239\n",
            "547.15564\n",
            "757.93164\n",
            "498.92325\n",
            "746.24866\n",
            "537.89594\n",
            "754.64746\n",
            "417.29755\n",
            "767.65814\n",
            "790.4767\n",
            "737.29694\n",
            "564.2343\n",
            "735.005\n",
            "671.645\n",
            "766.82825\n",
            "524.9593\n",
            "756.3949\n",
            "618.4\n",
            "759.83435\n",
            "472.47626\n",
            "762.1105\n",
            "444.09708\n",
            "771.48645\n",
            "624.9315\n",
            "777.1065\n",
            "541.74194\n",
            "758.7746\n",
            "531.084\n",
            "742.94696\n",
            "552.7586\n",
            "750.93036\n",
            "662.6724\n",
            "765.41144\n",
            "547.3021\n",
            "735.0179\n",
            "517.7299\n",
            "723.0262\n",
            "678.0079\n",
            "749.2058\n",
            "697.46814\n",
            "765.15533\n",
            "496.5868\n",
            "757.89246\n",
            "624.3987\n",
            "730.6144\n",
            "525.2265\n",
            "748.447\n",
            "497.73618\n",
            "758.82855\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:55<00:00, 11.12s/it, mean_val_loss=23.9]\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/tensor_util.py:536: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  append_fn(tensor_proto, proto_values)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "loading_vars_from inf_ne_5_lr_1e-06_bs_1000_t_0.1/model.ckpt-250\n",
            "INFO:tensorflow:Restoring parameters from inf_ne_5_lr_1e-06_bs_1000_t_0.1/model.ckpt-250\n",
            "[[ 2.4813963e-03 -1.3762937e-01 -4.4337213e-02  8.7590748e-04]\n",
            " [-1.3763013e-01  5.4693237e+01 -5.3892055e+00  6.8815053e-03]\n",
            " [-4.4336736e-02 -5.3891783e+00  6.0750785e+00  2.2168406e-03]\n",
            " [ 8.7590748e-04  6.8806354e-03  2.2163633e-03  9.5620204e-04]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BfLnWxOIOl8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRMcML617W-3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}